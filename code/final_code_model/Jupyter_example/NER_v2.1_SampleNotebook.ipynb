{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for Spanish Named Entity Recognition  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook based on: **Kamal Raj** NER with Bidirectional LSTM-CNNs implementation available on Github. https://github.com/kamalkraj/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs.\n",
    "\n",
    "\n",
    "**Versión: -v_1.2-**\n",
    "\n",
    "Notas de version:\n",
    "    \n",
    "    \n",
    "    - Se implementa las word embeddings en español: \n",
    "        GloVe embeddings for SWBC; #dimensions=300, #vectors=855380.\n",
    "    - Se modifico y mejoró el preprocesamiento de los datos de entrada para predicción. Ahora puede predecir las etiquetas I-(PER/LOC/ORG/MISC)\n",
    "    Para 50 epoch:\n",
    "    -Tiene un accuracy :~80 \n",
    "    -No se implementa nada para el español, se encontró que era perjudicial con las embeddings.\n",
    "    -Tiempo: 38 min aprox.  \n",
    "\n",
    "    Para 100 epoch: \n",
    "    -Tiene un accuracy :~84 (\n",
    "    -No se implementa nada para el español, se encontró que era perjudicial con las embeddings.\n",
    "    -Tiempo: 1 hr 20 min aprox.  \n",
    "   \n",
    " \n",
    "\n",
    "Entrenamiento realizado en:\n",
    "\n",
    "    DESKTOP-0UQLV13\n",
    "    Processor: Intel Core i7-6700HQ CPU 2.6GHz \n",
    "    RAM: 16GB\n",
    "    OS: Windows 10 Home Single x64\n",
    "    Tipo de memoria: SSD\n",
    "\n",
    "    \n",
    "\n",
    "Requiere:\n",
    "\n",
    "    unidecode\n",
    "    numpy (pip install --upgrade numpy)\n",
    "    nltk (pip install --upgrade nltk)\n",
    "    * Descargar nltk punkt y nltk stopwords:\n",
    "    * >> import nltk \n",
    "    * >> nltk.download('stopwords')\n",
    "    * >> nltk.download('punkt')\n",
    "    * Para más información: https://www.nltk.org/data.html \n",
    "    random\n",
    "    tensorflow 1.13.1 (pip install --upgrade tensorflow) *Actualmente (10/abril/19) no funciona con python 3.7.\n",
    "    * Para más información: https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class01_intro_python.ipynb \n",
    "    keras (pip install --upgrade keras) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NER task can be formulated as: \n",
    "\n",
    "_Given a sequence of tokens (words, and may be punctuation symbols) provide a tag from predefined set of tags for each token in the sequence._\n",
    "\n",
    "For NER task there are some common types of entities which essentially are tags:\n",
    "- Persons\n",
    "- Locations\n",
    "- Organizations\n",
    "- Expressions of time\n",
    "- Quantities\n",
    "- Monetary values \n",
    "\n",
    "Furthermore, to distinguish consequent entities with the same tags BIO tagging scheme is used. \"B\" stands for beginning, \n",
    "\"I\" stands for the continuation of an entity and \"O\" means the absence of entity. Example with dropped punctuation:\n",
    "\n",
    "    Bernhard        B-PER\n",
    "    Riemann         I-PER\n",
    "    Carl            B-PER\n",
    "    Friedrich       I-PER\n",
    "    Gauss           I-PER\n",
    "    and             O\n",
    "    Leonhard        B-PER\n",
    "    Euler           I-PER\n",
    "\n",
    "In the example above PER means person tag, and \"B-\" and \"I-\" are prefixes identifying beginnings and continuations of the entities. Without such prefixes, it is impossible to separate Bernhard Riemann from Carl Friedrich Gauss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Np for math \n",
    "# Keras for models, layers 4 NN layers \n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from keras.utils import Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import unidecode\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read file (txt) and divide the sentences into character bins (word, tag).\n",
    "def readfile(filename):\n",
    "    '''\n",
    "    read file\n",
    "    return format :\n",
    "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "    '''\n",
    "    f = open(filename, encoding='utf-8-sig') # open the file. Update to fix 'ï»¿'\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "            if len(sentence) > 0:     \n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        #splits[0] = unidecode.unidecode(splits[0]) # Remove special characters from spanish\n",
    "        #splits[0] = splits[0].lower() # Lowercase the words \n",
    "        #splits[0] = splits[0].translate(str.maketrans('', '', string.punctuation)) # remove puntuation \n",
    "        splits[-1] = splits[-1].replace('\\n', '').replace('\\r', '') #Remove all line breaks from a long string of text\n",
    "        if splits[0] != '':\n",
    "            sentence.append([splits[0],splits[-1]])\n",
    "\n",
    "    if len(sentence) >0: \n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 3 sets ************************************************* PATH ************************************************\n",
    "# Dataset CoNLL 2002 for Spanish, wich is divided into train, test, valid (dev) sets. Each row contains a word and it's tag\n",
    "# https://github.com/teropa/nlp/tree/master/resources/corpora/conll2002 \n",
    "trainSentences = readfile(\"tidy_data/train.txt\")\n",
    "devSentences = readfile(\"tidy_data/valid.txt\")\n",
    "testSentences = readfile(\"tidy_data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8323\n"
     ]
    }
   ],
   "source": [
    "print(len(trainSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Melbourne', 'B-LOC'],\n",
       " ['(', 'O'],\n",
       " ['Australia', 'B-LOC'],\n",
       " [')', 'O'],\n",
       " [',', 'O'],\n",
       " ['25', 'O'],\n",
       " ['may', 'O'],\n",
       " ['(', 'O'],\n",
       " ['EFE', 'B-ORG'],\n",
       " [')', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sao', 'B-LOC'],\n",
       " ['Paulo', 'I-LOC'],\n",
       " ['(', 'O'],\n",
       " ['Brasil', 'B-LOC'],\n",
       " [')', 'O'],\n",
       " [',', 'O'],\n",
       " ['23', 'O'],\n",
       " ['may', 'O'],\n",
       " ['(', 'O'],\n",
       " ['EFECOM', 'B-ORG'],\n",
       " [')', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['La', 'B-LOC'],\n",
       " ['Coruña', 'I-LOC'],\n",
       " [',', 'O'],\n",
       " ['23', 'O'],\n",
       " ['may', 'O'],\n",
       " ['(', 'O'],\n",
       " ['EFECOM', 'B-ORG'],\n",
       " [')', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new attribute in the character bins for padding\n",
    "def addCharInformatioin(Sentences):\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        for j,data in enumerate(sentence):\n",
    "            chars = [c for c in data[0]]\n",
    "            Sentences[i][j] = [data[0],chars,data[1]]\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = addCharInformatioin(trainSentences)\n",
    "devSentences = addCharInformatioin(devSentences)\n",
    "testSentences = addCharInformatioin(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Melbourne', ['M', 'e', 'l', 'b', 'o', 'u', 'r', 'n', 'e'], 'B-LOC'],\n",
       " ['(', ['('], 'O'],\n",
       " ['Australia', ['A', 'u', 's', 't', 'r', 'a', 'l', 'i', 'a'], 'B-LOC'],\n",
       " [')', [')'], 'O'],\n",
       " [',', [','], 'O'],\n",
       " ['25', ['2', '5'], 'O'],\n",
       " ['may', ['m', 'a', 'y'], 'O'],\n",
       " ['(', ['('], 'O'],\n",
       " ['EFE', ['E', 'F', 'E'], 'B-ORG'],\n",
       " [')', [')'], 'O'],\n",
       " ['.', ['.'], 'O']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sao', ['S', 'a', 'o'], 'B-LOC'],\n",
       " ['Paulo', ['P', 'a', 'u', 'l', 'o'], 'I-LOC'],\n",
       " ['(', ['('], 'O'],\n",
       " ['Brasil', ['B', 'r', 'a', 's', 'i', 'l'], 'B-LOC'],\n",
       " [')', [')'], 'O'],\n",
       " [',', [','], 'O'],\n",
       " ['23', ['2', '3'], 'O'],\n",
       " ['may', ['m', 'a', 'y'], 'O'],\n",
       " ['(', ['('], 'O'],\n",
       " ['EFECOM', ['E', 'F', 'E', 'C', 'O', 'M'], 'B-ORG'],\n",
       " [')', [')'], 'O'],\n",
       " ['.', ['.'], 'O']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['La', ['L', 'a'], 'B-LOC'],\n",
       " ['Coruña', ['C', 'o', 'r', 'u', 'ñ', 'a'], 'I-LOC'],\n",
       " [',', [','], 'O'],\n",
       " ['23', ['2', '3'], 'O'],\n",
       " ['may', ['m', 'a', 'y'], 'O'],\n",
       " ['(', ['('], 'O'],\n",
       " ['EFECOM', ['E', 'F', 'E', 'C', 'O', 'M'], 'B-ORG'],\n",
       " [')', [')'], 'O'],\n",
       " ['.', ['.'], 'O']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Creates the label set ( tag's set)\n",
    "# 2.Creates a set with the lowercased words contained in the train,dev,test sets \n",
    "labelSet = set()\n",
    "words = {}\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token,char,label in sentence:\n",
    "            labelSet.add(label)\n",
    "            words[token.lower()] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'melbourne': True,\n",
       " '(': True,\n",
       " 'australia': True,\n",
       " ')': True,\n",
       " ',': True,\n",
       " '25': True,\n",
       " 'may': True,\n",
       " 'efe': True,\n",
       " '.': True,\n",
       " '-': True,\n",
       " 'el': True,\n",
       " 'abogado': True,\n",
       " 'general': True,\n",
       " 'del': True,\n",
       " 'estado': True,\n",
       " 'daryl': True,\n",
       " 'williams': True,\n",
       " 'subrayó': True,\n",
       " 'hoy': True,\n",
       " 'la': True,\n",
       " 'necesidad': True,\n",
       " 'de': True,\n",
       " 'tomar': True,\n",
       " 'medidas': True,\n",
       " 'para': True,\n",
       " 'proteger': True,\n",
       " 'al': True,\n",
       " 'sistema': True,\n",
       " 'judicial': True,\n",
       " 'australiano': True,\n",
       " 'frente': True,\n",
       " 'a': True,\n",
       " 'una': True,\n",
       " 'página': True,\n",
       " 'internet': True,\n",
       " 'que': True,\n",
       " 'imposibilita': True,\n",
       " 'cumplimiento': True,\n",
       " 'los': True,\n",
       " 'principios': True,\n",
       " 'básicos': True,\n",
       " 'ley': True,\n",
       " 'petición': True,\n",
       " 'tiene': True,\n",
       " 'lugar': True,\n",
       " 'después': True,\n",
       " 'un': True,\n",
       " 'juez': True,\n",
       " 'tribunal': True,\n",
       " 'supremo': True,\n",
       " 'victoria': True,\n",
       " 'se': True,\n",
       " 'viera': True,\n",
       " 'forzado': True,\n",
       " 'disolver': True,\n",
       " 'jurado': True,\n",
       " 'popular': True,\n",
       " 'y': True,\n",
       " 'suspender': True,\n",
       " 'proceso': True,\n",
       " 'ante': True,\n",
       " 'argumento': True,\n",
       " 'defensa': True,\n",
       " 'las': True,\n",
       " 'personas': True,\n",
       " 'lo': True,\n",
       " 'componían': True,\n",
       " 'podían': True,\n",
       " 'haber': True,\n",
       " 'obtenido': True,\n",
       " 'información': True,\n",
       " 'sobre': True,\n",
       " 'acusado': True,\n",
       " 'través': True,\n",
       " 'crimenet': True,\n",
       " 'esta': True,\n",
       " 'web': True,\n",
       " 'lleva': True,\n",
       " 'mes': True,\n",
       " 'existencia': True,\n",
       " 'tiempo': True,\n",
       " 'en': True,\n",
       " 'ha': True,\n",
       " 'sido': True,\n",
       " 'visitada': True,\n",
       " 'más': True,\n",
       " 'millón': True,\n",
       " 'ocasiones': True,\n",
       " 'facilita': True,\n",
       " 'miles': True,\n",
       " 'crímenes': True,\n",
       " 'criminales': True,\n",
       " 'ya': True,\n",
       " 'enjuiciados': True,\n",
       " 'o': True,\n",
       " 'aún': True,\n",
       " 'perseguidos': True,\n",
       " 'datos': True,\n",
       " 'salen': True,\n",
       " 'artículos': True,\n",
       " 'periódicos': True,\n",
       " 'archivos': True,\n",
       " 'judiciales': True,\n",
       " 'por': True,\n",
       " 'su': True,\n",
       " 'parte': True,\n",
       " 'rob': True,\n",
       " 'hulls': True,\n",
       " 'indicó': True,\n",
       " 'no': True,\n",
       " 'hay': True,\n",
       " 'nadie': True,\n",
       " 'controle': True,\n",
       " 'informaciones': True,\n",
       " 'contenidas': True,\n",
       " 'son': True,\n",
       " 'veraces': True,\n",
       " 'señaló': True,\n",
       " 'jurídico': True,\n",
       " 'commonwealth': True,\n",
       " 'basa': True,\n",
       " 'justicia': True,\n",
       " 'australiana': True,\n",
       " 'es': True,\n",
       " 'fundamental': True,\n",
       " 'persona': True,\n",
       " 'sea': True,\n",
       " 'juzgada': True,\n",
       " 'únicamente': True,\n",
       " 'teniendo': True,\n",
       " 'cuenta': True,\n",
       " 'pruebas': True,\n",
       " 'presentadas': True,\n",
       " 'citada': True,\n",
       " 'considera': True,\n",
       " 'también': True,\n",
       " 'causa': True,\n",
       " 'conclusión': True,\n",
       " 'prematura': True,\n",
       " 'caso': True,\n",
       " 'asesinato': True,\n",
       " 'juzgado': True,\n",
       " 'recientemente': True,\n",
       " 'ciudad': True,\n",
       " 'capital': True,\n",
       " 'santander': True,\n",
       " 'izquierda': True,\n",
       " 'unida': True,\n",
       " 'presentó': True,\n",
       " 'nuevo': True,\n",
       " 'boletín': True,\n",
       " 'trimestral': True,\n",
       " '\"': True,\n",
       " 'publicación': True,\n",
       " 'distribuirán': True,\n",
       " '10.000': True,\n",
       " 'ejemplares': True,\n",
       " 'preferentemente': True,\n",
       " 'barrios': True,\n",
       " 'municipio': True,\n",
       " 'donde': True,\n",
       " 'colación': True,\n",
       " 'con': True,\n",
       " 'mayor': True,\n",
       " 'respaldo': True,\n",
       " 'ciudadano': True,\n",
       " 'portavoz': True,\n",
       " 'consejo': True,\n",
       " 'político': True,\n",
       " 'municipal': True,\n",
       " 'iu': True,\n",
       " 'ernesto': True,\n",
       " 'gómez': True,\n",
       " 'hera': True,\n",
       " 'explicó': True,\n",
       " 'conferencia': True,\n",
       " 'prensa': True,\n",
       " 'este': True,\n",
       " 'persigue': True,\n",
       " 'informar': True,\n",
       " 'abrir': True,\n",
       " 'espacios': True,\n",
       " 'debate': True,\n",
       " 'servir': True,\n",
       " 'como': True,\n",
       " 'semilla': True,\n",
       " 'movilización': True,\n",
       " 'política': True,\n",
       " 'comentó': True,\n",
       " 'contenidos': True,\n",
       " 'responden': True,\n",
       " 'programa': True,\n",
       " 'él': True,\n",
       " 'encabezó': True,\n",
       " 'candidato': True,\n",
       " 'elecciones': True,\n",
       " 'municipales': True,\n",
       " '1999': True,\n",
       " 'perdió': True,\n",
       " 'representación': True,\n",
       " 'ayuntamiento': True,\n",
       " 'confió': True,\n",
       " 'difusión': True,\n",
       " 'amplia': True,\n",
       " 'vez': True,\n",
       " 'coalición': True,\n",
       " 'resuelva': True,\n",
       " 'crisis': True,\n",
       " 'económica': True,\n",
       " 'momento': True,\n",
       " 'será': True,\n",
       " 'repartida': True,\n",
       " 'forma': True,\n",
       " 'preferencial': True,\n",
       " 'electores': True,\n",
       " 'entre': True,\n",
       " 'citó': True,\n",
       " 'ladera': True,\n",
       " 'norte': True,\n",
       " 'calle': True,\n",
       " 'alta': True,\n",
       " 'cazoña': True,\n",
       " 'zona': True,\n",
       " 'comprendida': True,\n",
       " 'calles': True,\n",
       " 'castilla': True,\n",
       " 'hermida': True,\n",
       " 'compuesto': True,\n",
       " 'dos': True,\n",
       " 'folios': True,\n",
       " 'plegados': True,\n",
       " 'mitad': True,\n",
       " 'sale': True,\n",
       " 'primer': True,\n",
       " 'número': True,\n",
       " 'criticando': True,\n",
       " 'obras': True,\n",
       " 'construcción': True,\n",
       " 'aparcamientos': True,\n",
       " 'subterráneos': True,\n",
       " 'molestias': True,\n",
       " 'están': True,\n",
       " 'causando': True,\n",
       " 'vecinos': True,\n",
       " 'modo': True,\n",
       " 'explotarán': True,\n",
       " 'pp': True,\n",
       " 'hacer': True,\n",
       " 'dinero': True,\n",
       " 'absolutamente': True,\n",
       " 'todo': True,\n",
       " 'ese': True,\n",
       " 'distribuya': True,\n",
       " 'sus': True,\n",
       " 'amigos': True,\n",
       " 'convertido': True,\n",
       " 'trampa': True,\n",
       " 'llena': True,\n",
       " 'agujeros': True,\n",
       " 'cuando': True,\n",
       " 'todas': True,\n",
       " 'ciudades': True,\n",
       " 'europa': True,\n",
       " 'occidental': True,\n",
       " 'tratando': True,\n",
       " 'cascos': True,\n",
       " 'urbanos': True,\n",
       " 'habitables': True,\n",
       " 'juicio': True,\n",
       " 'razones': True,\n",
       " 'llevar': True,\n",
       " 'adelante': True,\n",
       " 'estas': True,\n",
       " 'crematísticas': True,\n",
       " 'mercenarias': True,\n",
       " 'pero': True,\n",
       " 'tienen': True,\n",
       " 'nada': True,\n",
       " 'ver': True,\n",
       " 'futuro': True,\n",
       " 'ni': True,\n",
       " 'bienestar': True,\n",
       " 'inmensa': True,\n",
       " 'mayoría': True,\n",
       " 'población': True,\n",
       " 'cantabria': True,\n",
       " 'fráncfort': True,\n",
       " 'rfa': True,\n",
       " 'efecom': True,\n",
       " 'grupo': True,\n",
       " 'alemán': True,\n",
       " 'telefonía': True,\n",
       " 'deutsche': True,\n",
       " 'telekom': True,\n",
       " 'adquirido': True,\n",
       " '50': True,\n",
       " 'ciento': True,\n",
       " 'restante': True,\n",
       " 'operador': True,\n",
       " 'suizo': True,\n",
       " 'telecomunicaciones': True,\n",
       " 'multilink': True,\n",
       " 'hasta': True,\n",
       " 'ahora': True,\n",
       " 'poseía': True,\n",
       " 'france': True,\n",
       " 'telecom': True,\n",
       " 'único': True,\n",
       " 'propietario': True,\n",
       " 'informó': True,\n",
       " 'hoy,': True,\n",
       " 'jueves': True,\n",
       " 'compañía': True,\n",
       " 'germana': True,\n",
       " 'contrapartida': True,\n",
       " 'venderá': True,\n",
       " 'consorcio': True,\n",
       " 'francés': True,\n",
       " 'participación': True,\n",
       " 'empresa': True,\n",
       " 'mixta': True,\n",
       " 'británica': True,\n",
       " 'metroholdings': True,\n",
       " 'fue': True,\n",
       " 'creada': True,\n",
       " '1998': True,\n",
       " 'ofrece': True,\n",
       " 'servicios': True,\n",
       " 'telefónicos': True,\n",
       " 'transmisión': True,\n",
       " 'suiza': True,\n",
       " 'extranjero': True,\n",
       " 'además': True,\n",
       " 'tener': True,\n",
       " 'red': True,\n",
       " 'urbana': True,\n",
       " 'propia': True,\n",
       " 'fibra': True,\n",
       " 'óptica': True,\n",
       " 'calificó': True,\n",
       " 'compra': True,\n",
       " 'cuyo': True,\n",
       " 'precio': True,\n",
       " 'especificó': True,\n",
       " 'otro': True,\n",
       " 'paso': True,\n",
       " 'hacia': True,\n",
       " 'internacionalización': True,\n",
       " 'mediante': True,\n",
       " 'adquisiciones': True,\n",
       " 'mayoritarias': True,\n",
       " 'destinadas': True,\n",
       " 'control': True,\n",
       " 'dirección': True,\n",
       " 'esas': True,\n",
       " 'empresas': True,\n",
       " '============================================================': True,\n",
       " 'mérida': True,\n",
       " 'delegación': True,\n",
       " 'agencia': True,\n",
       " 'extremadura': True,\n",
       " 'transmitirá': True,\n",
       " 'jueves,': True,\n",
       " 'día': True,\n",
       " 'otras': True,\n",
       " 'siguientes': True,\n",
       " ':': True,\n",
       " 'lasa': True,\n",
       " 'zabala': True,\n",
       " 'presidente': True,\n",
       " 'junta': True,\n",
       " 'juan': True,\n",
       " 'carlos': True,\n",
       " 'rodríguez': True,\n",
       " 'ibarra': True,\n",
       " 'recibirá': True,\n",
       " 'sede': True,\n",
       " 'presidencia': True,\n",
       " 'gobierno': True,\n",
       " 'extremeño': True,\n",
       " 'familiares': True,\n",
       " 'varios': True,\n",
       " 'condenados': True,\n",
       " 'lasa-zabala': True,\n",
       " 'ellos': True,\n",
       " 'lourdes': True,\n",
       " 'díez': True,\n",
       " 'urraca': True,\n",
       " 'esposa': True,\n",
       " 'ex': True,\n",
       " 'gobernador': True,\n",
       " 'civil': True,\n",
       " 'guipúzcoa': True,\n",
       " 'julen': True,\n",
       " 'elgorriaga': True,\n",
       " ';': True,\n",
       " 'antonio': True,\n",
       " 'galindo': True,\n",
       " 'hermano': True,\n",
       " 'enrique': True,\n",
       " 'fotografía': True,\n",
       " 'pleno': True,\n",
       " 'asamblea': True,\n",
       " 'debatirá': True,\n",
       " 'proposición': True,\n",
       " 'grupos': True,\n",
       " 'psoe-ni': True,\n",
       " 'mixto-diputados': True,\n",
       " 'insta': True,\n",
       " 'central': True,\n",
       " 'promulgar': True,\n",
       " 'orden': True,\n",
       " 'ministerial': True,\n",
       " 'recoja': True,\n",
       " 'regionalización': True,\n",
       " 'superficie': True,\n",
       " 'base': True,\n",
       " 'cultivo': True,\n",
       " 'maíz': True,\n",
       " 'emprendedores': True,\n",
       " 'consejero': True,\n",
       " 'economía,': True,\n",
       " 'industria.': True,\n",
       " 'comercio': True,\n",
       " 'manuel': True,\n",
       " 'amigo': True,\n",
       " 'presidentes': True,\n",
       " 'caja': True,\n",
       " 'badajoz': True,\n",
       " 'jesús': True,\n",
       " 'medina': True,\n",
       " 'josé': True,\n",
       " 'sánchez': True,\n",
       " 'rojas': True,\n",
       " 'respectivamente': True,\n",
       " 'suscribirán': True,\n",
       " 'convenio': True,\n",
       " 'colaboración': True,\n",
       " 'financiar': True,\n",
       " 'proyectos': True,\n",
       " 'empresariales': True,\n",
       " 'jóvenes': True,\n",
       " 'v': True,\n",
       " 'congreso': True,\n",
       " 'jaraíz': True,\n",
       " 'vera': True,\n",
       " 'cáceres': True,\n",
       " 'instituto': True,\n",
       " 'histórico': True,\n",
       " 'hoffmeyer': True,\n",
       " 'acogerá': True,\n",
       " 'inauguración': True,\n",
       " 'primera': True,\n",
       " 'sesión': True,\n",
       " 'historia': True,\n",
       " 'moderna': True,\n",
       " 'carlos.': True,\n",
       " 'fin': True,\n",
       " 'época.': True,\n",
       " '1500-1558': True,\n",
       " 'enmarca': True,\n",
       " 'actos': True,\n",
       " 'conmemorativos': True,\n",
       " 'centenario': True,\n",
       " 'nacimiento': True,\n",
       " 'emperador': True,\n",
       " 'agenda': True,\n",
       " 'informativa': True,\n",
       " '10:00': True,\n",
       " 'h.': True,\n",
       " 'comenzará': True,\n",
       " 'vivienda,': True,\n",
       " 'urbanismo.': True,\n",
       " 'transportes': True,\n",
       " 'javier': True,\n",
       " 'corominas': True,\n",
       " 'entregará': True,\n",
       " '94': True,\n",
       " 'viviendas': True,\n",
       " 'régimen': True,\n",
       " 'especial': True,\n",
       " 'construidas': True,\n",
       " 'magenta': True,\n",
       " 'corchera': True,\n",
       " '10:30': True,\n",
       " 'plasencia': True,\n",
       " 'alcalde': True,\n",
       " 'luis': True,\n",
       " 'hablará': True,\n",
       " 'diversas': True,\n",
       " 'cuestiones': True,\n",
       " 'interés': True,\n",
       " '11:00': True,\n",
       " 'celebrará': True,\n",
       " '12:00': True,\n",
       " 'montijo': True,\n",
       " 'cultura': True,\n",
       " 'francisco': True,\n",
       " 'muñoz': True,\n",
       " 'inaugurará': True,\n",
       " 'iv': True,\n",
       " 'feria': True,\n",
       " 'libro': True,\n",
       " 'local': True,\n",
       " 'azuaga': True,\n",
       " 'xix': True,\n",
       " 'muestras': True,\n",
       " 'campiña': True,\n",
       " 'sur': True,\n",
       " 'fecsur': True,\n",
       " '12:30': True,\n",
       " 'recalificación': True,\n",
       " 'terrenos': True,\n",
       " 'aledaños': True,\n",
       " 'colegio': True,\n",
       " 'salesianos': True,\n",
       " '20:30': True,\n",
       " 'asistirá': True,\n",
       " 'presentación': True,\n",
       " 'seres.': True,\n",
       " 'estrellas': True,\n",
       " 'analiza': True,\n",
       " 'orígenes': True,\n",
       " 'universo': True,\n",
       " 'proyecto': True,\n",
       " 'investigación': True,\n",
       " 'primeros': True,\n",
       " 'pobladores': True,\n",
       " 'televisión': True,\n",
       " 'pública': True,\n",
       " 'sociedad': True,\n",
       " 'financiación': True,\n",
       " 'autonómica': True,\n",
       " 'jornada': True,\n",
       " 'extraescolar': True,\n",
       " 'creación': True,\n",
       " 'consultivo': True,\n",
       " 'asuntos': True,\n",
       " 'negociarán': True,\n",
       " 'partir': True,\n",
       " 'mañana': True,\n",
       " 'alcanzar': True,\n",
       " 'pacto': True,\n",
       " 'regional': True,\n",
       " '13': True,\n",
       " 'continuarán': True,\n",
       " 'actividades': True,\n",
       " 'agora.': True,\n",
       " 'peninsular': True,\n",
       " 'caius': True,\n",
       " 'apicius': True,\n",
       " 'madrid': True,\n",
       " 'vicio': True,\n",
       " 'debe': True,\n",
       " 'incurrir': True,\n",
       " 'jamás': True,\n",
       " 'quien': True,\n",
       " 'tenga': True,\n",
       " 'gastrónomo': True,\n",
       " 'aspire': True,\n",
       " 'ser': True,\n",
       " 'reconocido': True,\n",
       " 'tal': True,\n",
       " 'patriotería': True,\n",
       " 'entendida': True,\n",
       " 'alarde': True,\n",
       " 'excesivo': True,\n",
       " 'e': True,\n",
       " 'inoportuno': True,\n",
       " 'patriotismo': True,\n",
       " 'apátrida': True,\n",
       " 'huir': True,\n",
       " 'peste': True,\n",
       " 'chovinismo': True,\n",
       " 'hace': True,\n",
       " 'años': True,\n",
       " 'uno': True,\n",
       " 'libros': True,\n",
       " 'viaje': True,\n",
       " 'nobel': True,\n",
       " 'literatura': True,\n",
       " 'camilo': True,\n",
       " 'cela': True,\n",
       " 'afirmaba': True,\n",
       " 'patriota': True,\n",
       " 'está': True,\n",
       " 'orgulloso': True,\n",
       " 'nacido': True,\n",
       " 'defiende': True,\n",
       " 'actitud': True,\n",
       " 'razón': True,\n",
       " 'mientras': True,\n",
       " 'patriotero': True,\n",
       " 'entonces': True,\n",
       " 'usaba': True,\n",
       " 'palabra': True,\n",
       " \"'\": True,\n",
       " 'nacionalista': True,\n",
       " 'proclama': True,\n",
       " 'contra': True,\n",
       " 'viento.': True,\n",
       " 'marea': True,\n",
       " 'tierra': True,\n",
       " 'natal': True,\n",
       " 'superior': True,\n",
       " 'demás': True,\n",
       " 'cosa': True,\n",
       " 'decía': True,\n",
       " 'bien': True,\n",
       " 'pudiera': True,\n",
       " 'esto': True,\n",
       " 'comida': True,\n",
       " 'abunda': True,\n",
       " 'demasiado': True,\n",
       " 'ningún': True,\n",
       " 'inconveniente': True,\n",
       " 'aceptar': True,\n",
       " 'cada': True,\n",
       " 'cual': True,\n",
       " 'defienda': True,\n",
       " 'cocina': True,\n",
       " 'productos': True,\n",
       " '...': True,\n",
       " 'siempre': True,\n",
       " 'esa': True,\n",
       " 'implique': True,\n",
       " 'desprecio': True,\n",
       " 'cuanto': True,\n",
       " 'ajeno': True,\n",
       " 'costumbres': True,\n",
       " 'gastronómicas': True,\n",
       " 'solar': True,\n",
       " 'patrio': True,\n",
       " 'habrá': True,\n",
       " 'pasar': True,\n",
       " 'machado': True,\n",
       " 'recordar': True,\n",
       " 'aquello': True,\n",
       " 'desprecia': True,\n",
       " 'ignora': True,\n",
       " 'problema': True,\n",
       " 'gastronómica': True,\n",
       " 'nos': True,\n",
       " 'engañemos': True,\n",
       " 'cualquier': True,\n",
       " 'otra': True,\n",
       " 'ése': True,\n",
       " 'justamente': True,\n",
       " 'ignorancia': True,\n",
       " 'claro': True,\n",
       " 'misma': True,\n",
       " 'puede': True,\n",
       " 'caer': True,\n",
       " 'muchos': True,\n",
       " 'defecto': True,\n",
       " 'contrario': True,\n",
       " 'digamos': True,\n",
       " 'papanatismo': True,\n",
       " 'estos': True,\n",
       " 'ciudadanos': True,\n",
       " 'mejor': True,\n",
       " 'pues': True,\n",
       " 'tampoco': True,\n",
       " 'algo': True,\n",
       " 'así': True,\n",
       " 'viajero': True,\n",
       " 'turista': True,\n",
       " 'estilo': True,\n",
       " 'siglo': True,\n",
       " 'curiosidad': True,\n",
       " 'liberada': True,\n",
       " 'posible': True,\n",
       " 'prejuicios': True,\n",
       " 'dispuesta': True,\n",
       " 'cosas': True,\n",
       " 'buenas': True,\n",
       " 'sin': True,\n",
       " 'ello': True,\n",
       " 'renegar': True,\n",
       " 'propias': True,\n",
       " 'universalidad': True,\n",
       " 'universalizaron': True,\n",
       " 'gastronomía': True,\n",
       " 'fueron': True,\n",
       " 'romanos': True,\n",
       " 'implica': True,\n",
       " 'renuncia': True,\n",
       " 'propio': True,\n",
       " 'mucho': True,\n",
       " 'menos': True,\n",
       " 'mezcolanzas': True,\n",
       " 'absurdas': True,\n",
       " 'llamamos': True,\n",
       " 'fusión': True,\n",
       " 'muy': True,\n",
       " 'defendible': True,\n",
       " 'siempre.': True,\n",
       " 'derive': True,\n",
       " 'tantos': True,\n",
       " 'casos': True,\n",
       " 'confusión': True,\n",
       " 'insistiremos': True,\n",
       " 'verdad': True,\n",
       " 'grande': True,\n",
       " 'fiel': True,\n",
       " 'cómo': True,\n",
       " 'estar': True,\n",
       " 'abierta': True,\n",
       " 'incorporaciones': True,\n",
       " 'foráneas': True,\n",
       " 'qué': True,\n",
       " 'sería': True,\n",
       " 'europea': True,\n",
       " 'ejemplo': True,\n",
       " 'patatas': True,\n",
       " 'mediterránea': True,\n",
       " 'tomates': True,\n",
       " 'ambos': True,\n",
       " 'llegaron': True,\n",
       " 'viejo': True,\n",
       " 'mundo': True,\n",
       " 'cinco': True,\n",
       " 'siglos': True,\n",
       " 'imagínense': True,\n",
       " 'ustedes': True,\n",
       " 'aquellos': True,\n",
       " 'españoles': True,\n",
       " 'quienes': True,\n",
       " 'llevaron': True,\n",
       " 'esos': True,\n",
       " 'dones': True,\n",
       " 'americanos': True,\n",
       " 'hubiera': True,\n",
       " 'impuesto': True,\n",
       " 'hubieran': True,\n",
       " 'quedado': True,\n",
       " 'curiosidades': True,\n",
       " 'botánicas': True,\n",
       " 'universal': True,\n",
       " 'recuerden': True,\n",
       " 'horror': True,\n",
       " 'conocido': True,\n",
       " 'internacional': True,\n",
       " 'tan': True,\n",
       " 'frecuente': True,\n",
       " 'restaurantes': True,\n",
       " 'hotel': True,\n",
       " 'encerrada': True,\n",
       " 'sí': True,\n",
       " 'cabe': True,\n",
       " 'preguntarse': True,\n",
       " 'alimento': True,\n",
       " 'nueva': True,\n",
       " 'especia': True,\n",
       " '¿': True,\n",
       " 'dónde': True,\n",
       " 'viene': True,\n",
       " '?': True,\n",
       " 'investigar': True,\n",
       " 'antes': True,\n",
       " 'verdaderamente': True,\n",
       " 'importante': True,\n",
       " 'rico': True,\n",
       " 'si': True,\n",
       " 'viniere': True,\n",
       " 'supuesto': True,\n",
       " 'estudiar': True,\n",
       " 'adaptamos': True,\n",
       " 'producto': True,\n",
       " 'nuestros': True,\n",
       " 'usos': True,\n",
       " 'culinarios': True,\n",
       " 'arraigados': True,\n",
       " 'nuestra': True,\n",
       " 'evoluciona': True,\n",
       " 'enriquece': True,\n",
       " 'manera': True,\n",
       " 'encerrándose': True,\n",
       " 'acaba': True,\n",
       " 'debilitarse': True,\n",
       " 'extinguirse': True,\n",
       " 'endogamia': True,\n",
       " 'buena': True,\n",
       " 'casi': True,\n",
       " 'tengan': True,\n",
       " 'mente': True,\n",
       " 'sentidos': True,\n",
       " 'todavía': True,\n",
       " 'parafraseando': True,\n",
       " 'shakespeare': True,\n",
       " 'hamlet': True,\n",
       " 'ricas': True,\n",
       " 'alcanza': True,\n",
       " 'tu': True,\n",
       " 'sabiduría': True,\n",
       " 'experiencia': True,\n",
       " 'personal': True,\n",
       " 'conviertan': True,\n",
       " 'eunucos': True,\n",
       " 'gusto': True,\n",
       " 'mera': True,\n",
       " 'perderán': True,\n",
       " 'tantas': True,\n",
       " 'alcorcón': True,\n",
       " 'museo': True,\n",
       " 'arte': True,\n",
       " 'vidrio': True,\n",
       " 'acoge': True,\n",
       " 'desde': True,\n",
       " 'próximo': True,\n",
       " '28': True,\n",
       " 'junio': True,\n",
       " 'exposición': True,\n",
       " 'esculturas': True,\n",
       " 'artista': True,\n",
       " 'rumano': True,\n",
       " 'edward': True,\n",
       " 'leibovtiz': True,\n",
       " 'compuesta': True,\n",
       " '35': True,\n",
       " 'piezas': True,\n",
       " 'diferentes': True,\n",
       " 'formatos': True,\n",
       " 'tamaño': True,\n",
       " 'oscila': True,\n",
       " '40': True,\n",
       " 'centímetros': True,\n",
       " '2': True,\n",
       " 'metros': True,\n",
       " 'algunas': True,\n",
       " 'figuras': True,\n",
       " 'muestra': True,\n",
       " 'organizada': True,\n",
       " 'concejalía': True,\n",
       " 'carácter': True,\n",
       " 'figurativo': True,\n",
       " 'ellas': True,\n",
       " 'aprecian': True,\n",
       " 'caras': True,\n",
       " 'cuerpos': True,\n",
       " 'distintas': True,\n",
       " 'fantasías': True,\n",
       " 'mostrando': True,\n",
       " 'incluso': True,\n",
       " 'aspectos': True,\n",
       " 'eróticos': True,\n",
       " 'leibovitz': True,\n",
       " 'reside': True,\n",
       " 'bélgica': True,\n",
       " 'actualidad': True,\n",
       " 'expuesto': True,\n",
       " '30': True,\n",
       " 'trabajo': True,\n",
       " 'escultor': True,\n",
       " 'contemporáneo': True,\n",
       " 'numerosos': True,\n",
       " 'museos': True,\n",
       " 'europeos': True,\n",
       " 'asiáticos': True,\n",
       " 'países': True,\n",
       " 'japón': True,\n",
       " 'alemania': True,\n",
       " 'francia': True,\n",
       " 'israel': True,\n",
       " 'asistentes': True,\n",
       " 'tarde': True,\n",
       " 'podrán': True,\n",
       " 'disfrutar': True,\n",
       " 'señala': True,\n",
       " 'me': True,\n",
       " 'gusta': True,\n",
       " 'trabajar': True,\n",
       " 'composición': True,\n",
       " 'pieza': True,\n",
       " 'trato': True,\n",
       " 'transmitir': True,\n",
       " 'fantasía': True,\n",
       " 'vida': True,\n",
       " 'aspecto': True,\n",
       " 'absurdo': True,\n",
       " 'prevista': True,\n",
       " '20': True,\n",
       " 'horas': True,\n",
       " 'concejal': True,\n",
       " 'torres': True,\n",
       " 'cuya': True,\n",
       " 'cursado': True,\n",
       " 'invitación': True,\n",
       " 'embajadores': True,\n",
       " 'rumanía': True,\n",
       " 'consulados': True,\n",
       " 'según': True,\n",
       " 'fuentes': True,\n",
       " 'han': True,\n",
       " 'confirmado': True,\n",
       " 'presencia': True,\n",
       " 'distintos': True,\n",
       " 'representantes': True,\n",
       " 'ampliación': True,\n",
       " 'máxima': True,\n",
       " 'terra': True,\n",
       " 'canje': True,\n",
       " 'acciones': True,\n",
       " 'lycos': True,\n",
       " 'incluidas': True,\n",
       " 'opciones': True,\n",
       " 'trabajadores': True,\n",
       " 'estadounidense': True,\n",
       " 'excederá': True,\n",
       " '340.922.712': True,\n",
       " 'cierre': True,\n",
       " 'ayer': True,\n",
       " 'bolsa': True,\n",
       " '50,9': True,\n",
       " 'pesetas': True,\n",
       " 'acción': True,\n",
       " '2,887': True,\n",
       " 'billones': True,\n",
       " 'consta': True,\n",
       " 'nota': True,\n",
       " 'aclaratoria': True,\n",
       " 'telefónica': True,\n",
       " 'comisión': True,\n",
       " 'nacional': True,\n",
       " 'mercado': True,\n",
       " 'valores': True,\n",
       " 'ampliaciones': True,\n",
       " 'serán': True,\n",
       " 'sometidas': True,\n",
       " 'aprobación': True,\n",
       " 'accionistas': True,\n",
       " 'cabo': True,\n",
       " 'operación': True,\n",
       " 'filial': True,\n",
       " 'network': True,\n",
       " 'portal': True,\n",
       " 'aclaración': True,\n",
       " 'indica': True,\n",
       " 'trata': True,\n",
       " 'máximas': True,\n",
       " 'cubrir': True,\n",
       " 'acuerdos': True,\n",
       " 'realizado': True,\n",
       " 'dice': True,\n",
       " 'americana': True,\n",
       " '112.192.815': True,\n",
       " 'previstas': True,\n",
       " 'adquisición': True,\n",
       " 'activos': True,\n",
       " 'importe': True,\n",
       " '9,7': True,\n",
       " 'millones': True,\n",
       " 'títulos': True,\n",
       " 'empleados': True,\n",
       " 'planes': True,\n",
       " '23.640.000': True,\n",
       " 'otros': True,\n",
       " '7.440.000': True,\n",
       " 'máximo': True,\n",
       " 'podrían': True,\n",
       " 'emitidas': True,\n",
       " 'total': True,\n",
       " '158.568.703': True,\n",
       " 'cambiarían': True,\n",
       " '2,15': True,\n",
       " 'española': True,\n",
       " 'emisión': True,\n",
       " '38.346.984': True,\n",
       " 'warrants': True,\n",
       " 'derechos': True,\n",
       " 'suponer': True,\n",
       " '82,45': True,\n",
       " 'texto': True,\n",
       " 'aclara': True,\n",
       " 'incluido': True,\n",
       " '35,48': True,\n",
       " 'someterá': True,\n",
       " 'supondrían': True,\n",
       " '300.514': True,\n",
       " 'realizarán': True,\n",
       " '62': True,\n",
       " 'euros': True,\n",
       " 'cotización': True,\n",
       " 'días': True,\n",
       " 'pactado': True,\n",
       " 'firma': True,\n",
       " 'acuerdo': True,\n",
       " 'hecho': True,\n",
       " 'cotice': True,\n",
       " 'actualmente': True,\n",
       " 'debajo': True,\n",
       " 'podría': True,\n",
       " 'provocar': True,\n",
       " 'operadora': True,\n",
       " 'acudan': True,\n",
       " 'obligaría': True,\n",
       " 'suscribir': True,\n",
       " 'figura': True,\n",
       " 'explica': True,\n",
       " 'resultado': True,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the labels a numerical id.\n",
    "# :: Create a mapping for the labels ::\n",
    "label2Idx = {}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-ORG': 0,\n",
       " 'I-ORG': 1,\n",
       " 'I-PER': 2,\n",
       " 'I-LOC': 3,\n",
       " 'B-LOC': 4,\n",
       " 'B-MISC': 5,\n",
       " 'B-PER': 6,\n",
       " 'O': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up table\n",
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numeric': 0,\n",
       " 'allLower': 1,\n",
       " 'allUpper': 2,\n",
       " 'initialUpper': 3,\n",
       " 'other': 4,\n",
       " 'mainly_numeric': 5,\n",
       " 'contains_digit': 6,\n",
       " 'PADDING_TOKEN': 7}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case2Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Read in word embeddings ::\n",
    "word2Idx = {}\n",
    "wordEmbeddings = []\n",
    "# *********************************************************************************************** PATH *************************************************\n",
    "# GloVe embeddings from SBWC\n",
    "# https://github.com/uchile-nlp/spanish-word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Hace los wordEmbedings en base a la lista de embedings + revisa si la palabra en embeddings esta contenido en la lista \n",
    "# de palabras ** Nota: Remember that the words are seen as vectors.\n",
    "with open(\"word_embeddings/SBW-vectors-300-min5.txt\", encoding=\"utf-8\") as fEmbeddings:  ## change to skip first line (headings)\n",
    "    next(fEmbeddings)\n",
    "    for line in fEmbeddings:\n",
    "        split = line.strip().split(' ')\n",
    "        word = split[0]\n",
    "\n",
    "        if len(word2Idx) == 0: #Add padding+unknown\n",
    "            word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "            vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
    "            wordEmbeddings.append(vector)\n",
    "\n",
    "            word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "            vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "            wordEmbeddings.append(vector)\n",
    "\n",
    "        if split[0].lower() in words:\n",
    "            vector = np.array([float(num) for num in split[1:]])\n",
    "            wordEmbeddings.append(vector)\n",
    "            word2Idx[split[0]] = len(word2Idx)\n",
    "\n",
    "    wordEmbeddings = np.array(wordEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.12417509,  0.08496462, -0.06926975, ..., -0.05228813,\n",
       "         0.0231604 , -0.18764412],\n",
       "       [-0.029648  ,  0.011336  ,  0.019949  , ..., -0.128057  ,\n",
       "        -0.004917  ,  0.062628  ],\n",
       "       ...,\n",
       "       [-0.005399  , -0.018904  ,  0.00199   , ..., -0.023052  ,\n",
       "         0.038082  ,  0.024057  ],\n",
       "       [-0.010136  , -0.020782  , -0.041017  , ..., -0.079891  ,\n",
       "        -0.037598  ,  0.017487  ],\n",
       "       [ 0.095675  , -0.068076  , -0.067965  , ..., -0.012334  ,\n",
       "         0.012556  ,  0.001487  ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54819"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEmbeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEmbeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2Idx = {\"PADDING\":0, \"UNKNOWN\":1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZáéíóúñäëïöüÁÉÍÓÚÄËÏÖÜÃÂñÑàèìòùÀÈÌÒÙ.,-_()[]{}¡!¿?:;#'\\\"/\\\\%$`&=*+@^~|‘´·»³©\\xadº±¼\\xa0\":\n",
    "    char2Idx[c] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PADDING': 0,\n",
       " 'UNKNOWN': 1,\n",
       " ' ': 2,\n",
       " '0': 3,\n",
       " '1': 4,\n",
       " '2': 5,\n",
       " '3': 6,\n",
       " '4': 7,\n",
       " '5': 8,\n",
       " '6': 9,\n",
       " '7': 10,\n",
       " '8': 11,\n",
       " '9': 12,\n",
       " 'a': 13,\n",
       " 'b': 14,\n",
       " 'c': 15,\n",
       " 'd': 16,\n",
       " 'e': 17,\n",
       " 'f': 18,\n",
       " 'g': 19,\n",
       " 'h': 20,\n",
       " 'i': 21,\n",
       " 'j': 22,\n",
       " 'k': 23,\n",
       " 'l': 24,\n",
       " 'm': 25,\n",
       " 'n': 26,\n",
       " 'o': 27,\n",
       " 'p': 28,\n",
       " 'q': 29,\n",
       " 'r': 30,\n",
       " 's': 31,\n",
       " 't': 32,\n",
       " 'u': 33,\n",
       " 'v': 34,\n",
       " 'w': 35,\n",
       " 'x': 36,\n",
       " 'y': 37,\n",
       " 'z': 38,\n",
       " 'A': 39,\n",
       " 'B': 40,\n",
       " 'C': 41,\n",
       " 'D': 42,\n",
       " 'E': 43,\n",
       " 'F': 44,\n",
       " 'G': 45,\n",
       " 'H': 46,\n",
       " 'I': 47,\n",
       " 'J': 48,\n",
       " 'K': 49,\n",
       " 'L': 50,\n",
       " 'M': 51,\n",
       " 'N': 52,\n",
       " 'O': 53,\n",
       " 'P': 54,\n",
       " 'Q': 55,\n",
       " 'R': 56,\n",
       " 'S': 57,\n",
       " 'T': 58,\n",
       " 'U': 59,\n",
       " 'V': 60,\n",
       " 'W': 61,\n",
       " 'X': 62,\n",
       " 'Y': 63,\n",
       " 'Z': 64,\n",
       " 'á': 65,\n",
       " 'é': 66,\n",
       " 'í': 67,\n",
       " 'ó': 68,\n",
       " 'ú': 69,\n",
       " 'ñ': 88,\n",
       " 'ä': 71,\n",
       " 'ë': 72,\n",
       " 'ï': 73,\n",
       " 'ö': 74,\n",
       " 'ü': 75,\n",
       " 'Á': 76,\n",
       " 'É': 77,\n",
       " 'Í': 78,\n",
       " 'Ó': 79,\n",
       " 'Ú': 80,\n",
       " 'Ä': 81,\n",
       " 'Ë': 82,\n",
       " 'Ï': 83,\n",
       " 'Ö': 84,\n",
       " 'Ü': 85,\n",
       " 'Ã': 86,\n",
       " 'Â': 87,\n",
       " 'Ñ': 88,\n",
       " 'à': 89,\n",
       " 'è': 90,\n",
       " 'ì': 91,\n",
       " 'ò': 92,\n",
       " 'ù': 93,\n",
       " 'À': 94,\n",
       " 'È': 95,\n",
       " 'Ì': 96,\n",
       " 'Ò': 97,\n",
       " 'Ù': 98,\n",
       " '.': 99,\n",
       " ',': 100,\n",
       " '-': 101,\n",
       " '_': 102,\n",
       " '(': 103,\n",
       " ')': 104,\n",
       " '[': 105,\n",
       " ']': 106,\n",
       " '{': 107,\n",
       " '}': 108,\n",
       " '¡': 109,\n",
       " '!': 110,\n",
       " '¿': 111,\n",
       " '?': 112,\n",
       " ':': 113,\n",
       " ';': 114,\n",
       " '#': 115,\n",
       " \"'\": 116,\n",
       " '\"': 117,\n",
       " '/': 118,\n",
       " '\\\\': 119,\n",
       " '%': 120,\n",
       " '$': 121,\n",
       " '`': 122,\n",
       " '&': 123,\n",
       " '=': 124,\n",
       " '*': 125,\n",
       " '+': 126,\n",
       " '@': 127,\n",
       " '^': 128,\n",
       " '~': 129,\n",
       " '|': 130,\n",
       " '‘': 131,\n",
       " '´': 132,\n",
       " '·': 133,\n",
       " '»': 134,\n",
       " '³': 135,\n",
       " '©': 136,\n",
       " '\\xad': 137,\n",
       " 'º': 138,\n",
       " '±': 139,\n",
       " '¼': 140,\n",
       " '\\xa0': 141}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# characters and position (val)\n",
    "char2Idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PADDING_TOKEN': 0,\n",
       " 'UNKNOWN_TOKEN': 1,\n",
       " 'de': 2,\n",
       " 'la': 3,\n",
       " 'en': 4,\n",
       " 'el': 5,\n",
       " 'y': 6,\n",
       " 'que': 7,\n",
       " 'a': 8,\n",
       " 'los': 9,\n",
       " 'del': 10,\n",
       " 'las': 11,\n",
       " 'se': 12,\n",
       " 'por': 13,\n",
       " 'un': 14,\n",
       " 'con': 15,\n",
       " 'para': 16,\n",
       " 'una': 17,\n",
       " 'su': 18,\n",
       " 'al': 19,\n",
       " 'no': 20,\n",
       " 'es': 21,\n",
       " 'El': 22,\n",
       " 'como': 23,\n",
       " 'La': 24,\n",
       " 'más': 25,\n",
       " 'En': 26,\n",
       " 'lo': 27,\n",
       " 'o': 28,\n",
       " 'sobre': 29,\n",
       " 'sus': 30,\n",
       " 'ha': 31,\n",
       " 'fue': 32,\n",
       " 'entre': 33,\n",
       " 'este': 34,\n",
       " 'Los': 35,\n",
       " 'también': 36,\n",
       " 'años': 37,\n",
       " 'dos': 38,\n",
       " 'pero': 39,\n",
       " 'son': 40,\n",
       " 'han': 41,\n",
       " 'esta': 42,\n",
       " 'le': 43,\n",
       " 'A': 44,\n",
       " 'parte': 45,\n",
       " 'ser': 46,\n",
       " 'Estados': 47,\n",
       " 'está': 48,\n",
       " 'ya': 49,\n",
       " 'año': 50,\n",
       " 'hasta': 51,\n",
       " 'desde': 52,\n",
       " 'contra': 53,\n",
       " 'sin': 54,\n",
       " 'e': 55,\n",
       " 'Se': 56,\n",
       " 'Las': 57,\n",
       " 'si': 58,\n",
       " 'todos': 59,\n",
       " 'cuando': 60,\n",
       " 'donde': 61,\n",
       " 'Comisión': 62,\n",
       " 'otros': 63,\n",
       " 'tiene': 64,\n",
       " 'durante': 65,\n",
       " 'todo': 66,\n",
       " 'países': 67,\n",
       " 'Naciones': 68,\n",
       " 'Unidas': 69,\n",
       " 'muy': 70,\n",
       " 'personas': 71,\n",
       " 'así': 72,\n",
       " 'Consejo': 73,\n",
       " 'puede': 74,\n",
       " 'desarrollo': 75,\n",
       " 'Por': 76,\n",
       " 'era': 77,\n",
       " 'General': 78,\n",
       " 'había': 79,\n",
       " 'país': 80,\n",
       " 'vez': 81,\n",
       " 'Estado': 82,\n",
       " 'tres': 83,\n",
       " 'sido': 84,\n",
       " 'derechos': 85,\n",
       " 'uno': 86,\n",
       " 'después': 87,\n",
       " 'No': 88,\n",
       " 'ese': 89,\n",
       " 'millones': 90,\n",
       " 'lugar': 91,\n",
       " 'cada': 92,\n",
       " 'forma': 93,\n",
       " 'mismo': 94,\n",
       " 'otras': 95,\n",
       " 'Gobierno': 96,\n",
       " 'información': 97,\n",
       " 'artículo': 98,\n",
       " 'gran': 99,\n",
       " 'primera': 100,\n",
       " 'primer': 101,\n",
       " 'tiempo': 102,\n",
       " 'medidas': 103,\n",
       " 'mayor': 104,\n",
       " 'tanto': 105,\n",
       " 'hay': 106,\n",
       " 'trabajo': 107,\n",
       " 'ciudad': 108,\n",
       " 'caso': 109,\n",
       " 'Comité': 110,\n",
       " 'Es': 111,\n",
       " 'están': 112,\n",
       " 'fueron': 113,\n",
       " 'miembros': 114,\n",
       " 'informe': 115,\n",
       " 'De': 116,\n",
       " 'sólo': 117,\n",
       " 'internacional': 118,\n",
       " 'programa': 119,\n",
       " 'período': 120,\n",
       " 'porque': 121,\n",
       " 'cual': 122,\n",
       " 'sistema': 123,\n",
       " 'cuenta': 124,\n",
       " 'ante': 125,\n",
       " 'hacer': 126,\n",
       " 'fin': 127,\n",
       " 'proyecto': 128,\n",
       " 'general': 129,\n",
       " 'antes': 130,\n",
       " 'derecho': 131,\n",
       " 'día': 132,\n",
       " 'vida': 133,\n",
       " 'todas': 134,\n",
       " 'embargo': 135,\n",
       " 'población': 136,\n",
       " 'seguridad': 137,\n",
       " 'grupo': 138,\n",
       " 'quien': 139,\n",
       " 'nuevo': 140,\n",
       " 'nombre': 141,\n",
       " 'menos': 142,\n",
       " 'hace': 143,\n",
       " 'hecho': 144,\n",
       " 'actividades': 145,\n",
       " 'debe': 146,\n",
       " 'según': 147,\n",
       " 'San': 148,\n",
       " 'mundo': 149,\n",
       " 'Unidos': 150,\n",
       " 'resolución': 151,\n",
       " 'bien': 152,\n",
       " 'Para': 153,\n",
       " 'esa': 154,\n",
       " 'situación': 155,\n",
       " 'aunque': 156,\n",
       " 'número': 157,\n",
       " 'equipo': 158,\n",
       " 'servicios': 159,\n",
       " 'dijo': 160,\n",
       " 'otro': 161,\n",
       " 'política': 162,\n",
       " 'Al': 163,\n",
       " 'tras': 164,\n",
       " 'respecto': 165,\n",
       " 'pueden': 166,\n",
       " 'estado': 167,\n",
       " 'estos': 168,\n",
       " 'nacional': 169,\n",
       " 'apoyo': 170,\n",
       " 'acuerdo': 171,\n",
       " 'España': 172,\n",
       " 'ni': 173,\n",
       " 'humanos': 174,\n",
       " 'presidente': 175,\n",
       " 'niños': 176,\n",
       " 'Nacional': 177,\n",
       " 'República': 178,\n",
       " 'eran': 179,\n",
       " 'aplicación': 180,\n",
       " 'importante': 181,\n",
       " 'tienen': 182,\n",
       " 'nivel': 183,\n",
       " 'algunos': 184,\n",
       " 'mujeres': 185,\n",
       " 'proceso': 186,\n",
       " 'momento': 187,\n",
       " 'les': 188,\n",
       " 'Sin': 189,\n",
       " 'me': 190,\n",
       " 'ellos': 191,\n",
       " 'Y': 192,\n",
       " 'Presidente': 193,\n",
       " 'También': 194,\n",
       " 'manera': 195,\n",
       " 'Su': 196,\n",
       " 'familia': 197,\n",
       " 'nos': 198,\n",
       " 'recursos': 199,\n",
       " 'sesiones': 200,\n",
       " 'además': 201,\n",
       " 'otra': 202,\n",
       " 'hoy': 203,\n",
       " 'Además': 204,\n",
       " 'México': 205,\n",
       " 'días': 206,\n",
       " 'mientras': 207,\n",
       " 'medio': 208,\n",
       " 'relación': 209,\n",
       " 'total': 210,\n",
       " 'mejor': 211,\n",
       " 'bajo': 212,\n",
       " 'presente': 213,\n",
       " 'cuatro': 214,\n",
       " 'pasado': 215,\n",
       " 'Asamblea': 216,\n",
       " 'diciembre': 217,\n",
       " 'Con': 218,\n",
       " 'él': 219,\n",
       " 'ahora': 220,\n",
       " 'unos': 221,\n",
       " 'ayer': 222,\n",
       " 'horas': 223,\n",
       " 'Este': 224,\n",
       " 'tipo': 225,\n",
       " 'sea': 226,\n",
       " 'hacia': 227,\n",
       " 'gobierno': 228,\n",
       " 'Unión': 229,\n",
       " 'zona': 230,\n",
       " 'programas': 231,\n",
       " 'final': 232,\n",
       " 'encuentra': 233,\n",
       " 'Un': 234,\n",
       " 'particular': 235,\n",
       " 'será': 236,\n",
       " 'tener': 237,\n",
       " 'José': 238,\n",
       " 'Internacional': 239,\n",
       " 'toda': 240,\n",
       " 'estaba': 241,\n",
       " 'Según': 242,\n",
       " 'Grupo': 243,\n",
       " 'estas': 244,\n",
       " 'junio': 245,\n",
       " 'Si': 246,\n",
       " 'cualquier': 247,\n",
       " 'personal': 248,\n",
       " 'sino': 249,\n",
       " 'partido': 250,\n",
       " 'mediante': 251,\n",
       " 'junto': 252,\n",
       " 'Secretario': 253,\n",
       " 'datos': 254,\n",
       " 'mi': 255,\n",
       " 'poder': 256,\n",
       " 'principal': 257,\n",
       " 'región': 258,\n",
       " 'frente': 259,\n",
       " 'siendo': 260,\n",
       " 'muchos': 261,\n",
       " 'varios': 262,\n",
       " 'nueva': 263,\n",
       " 'ejemplo': 264,\n",
       " 'internacionales': 265,\n",
       " 'Seguridad': 266,\n",
       " 'dentro': 267,\n",
       " 'social': 268,\n",
       " 'través': 269,\n",
       " 'base': 270,\n",
       " 'paz': 271,\n",
       " 'C': 272,\n",
       " 'Europa': 273,\n",
       " 'julio': 274,\n",
       " 'haber': 275,\n",
       " 'Una': 276,\n",
       " 'segundo': 277,\n",
       " 'posible': 278,\n",
       " 'organizaciones': 279,\n",
       " 'Esta': 280,\n",
       " 'condiciones': 281,\n",
       " 'sector': 282,\n",
       " 'mujer': 283,\n",
       " 'mucho': 284,\n",
       " 'Europea': 285,\n",
       " 'serie': 286,\n",
       " 'agua': 287,\n",
       " 'mil': 288,\n",
       " 'dólares': 289,\n",
       " 'siempre': 290,\n",
       " 'cinco': 291,\n",
       " 'mayo': 292,\n",
       " 'Pero': 293,\n",
       " 'ello': 294,\n",
       " 'último': 295,\n",
       " 'tema': 296,\n",
       " 'poco': 297,\n",
       " 'decisión': 298,\n",
       " 'punto': 299,\n",
       " 'marzo': 300,\n",
       " 'casos': 301,\n",
       " 'Juan': 302,\n",
       " 'capacidad': 303,\n",
       " 'público': 304,\n",
       " 'enero': 305,\n",
       " 'protección': 306,\n",
       " 'autoridades': 307,\n",
       " 'ley': 308,\n",
       " 'meses': 309,\n",
       " 'septiembre': 310,\n",
       " 'octubre': 311,\n",
       " 'problemas': 312,\n",
       " 'misma': 313,\n",
       " 'uso': 314,\n",
       " 'centro': 315,\n",
       " 'cooperación': 316,\n",
       " 'partir': 317,\n",
       " 'nacionales': 318,\n",
       " 'mayoría': 319,\n",
       " 'hizo': 320,\n",
       " 'noviembre': 321,\n",
       " 'deben': 322,\n",
       " 'tan': 323,\n",
       " 'solo': 324,\n",
       " 'esto': 325,\n",
       " 'asistencia': 326,\n",
       " 'grupos': 327,\n",
       " 'siglo': 328,\n",
       " 'debido': 329,\n",
       " 'siguiente': 330,\n",
       " 'abril': 331,\n",
       " 'grandes': 332,\n",
       " 'Madrid': 333,\n",
       " 'partes': 334,\n",
       " 'historia': 335,\n",
       " 'tenía': 336,\n",
       " 'Mundial': 337,\n",
       " 'aún': 338,\n",
       " 'productos': 339,\n",
       " 'municipio': 340,\n",
       " 'tuvo': 341,\n",
       " 'políticas': 342,\n",
       " 'salud': 343,\n",
       " 'largo': 344,\n",
       " 'I': 345,\n",
       " 'mercado': 346,\n",
       " 'armas': 347,\n",
       " 'cambio': 348,\n",
       " 'cuestión': 349,\n",
       " 'medios': 350,\n",
       " 'entonces': 351,\n",
       " 'Artículo': 352,\n",
       " 'esos': 353,\n",
       " 'cuestiones': 354,\n",
       " 'luego': 355,\n",
       " 'empresas': 356,\n",
       " 'atención': 357,\n",
       " 'The': 358,\n",
       " 'pues': 359,\n",
       " 'qué': 360,\n",
       " 'participación': 361,\n",
       " 'materia': 362,\n",
       " 'algunas': 363,\n",
       " 'Fue': 364,\n",
       " 'ella': 365,\n",
       " 'Nueva': 366,\n",
       " 'trata': 367,\n",
       " 'sí': 368,\n",
       " 'párrafo': 369,\n",
       " 'tal': 370,\n",
       " 'Reglamento': 371,\n",
       " 'actual': 372,\n",
       " 'segunda': 373,\n",
       " 'Como': 374,\n",
       " 'sociedad': 375,\n",
       " 'marco': 376,\n",
       " 'habían': 377,\n",
       " 'dar': 378,\n",
       " 'resultados': 379,\n",
       " 'objetivo': 380,\n",
       " 'fecha': 381,\n",
       " 'especial': 382,\n",
       " 'Universidad': 383,\n",
       " 'obra': 384,\n",
       " 'of': 385,\n",
       " 'cabo': 386,\n",
       " 'decir': 387,\n",
       " 'fuera': 388,\n",
       " 'acceso': 389,\n",
       " 'comunidad': 390,\n",
       " 'eso': 391,\n",
       " 'control': 392,\n",
       " 'puesto': 393,\n",
       " 'empresa': 394,\n",
       " 'casi': 395,\n",
       " 'producción': 396,\n",
       " 'principales': 397,\n",
       " 'incluso': 398,\n",
       " 'habitantes': 399,\n",
       " 'b': 400,\n",
       " 'the': 401,\n",
       " 'pueblo': 402,\n",
       " 'ciento': 403,\n",
       " 'casa': 404,\n",
       " 'investigación': 405,\n",
       " 'servicio': 406,\n",
       " 'puntos': 407,\n",
       " 'podría': 408,\n",
       " 'diferentes': 409,\n",
       " 'agosto': 410,\n",
       " 'capital': 411,\n",
       " 'varias': 412,\n",
       " 'educación': 413,\n",
       " 'febrero': 414,\n",
       " 'estar': 415,\n",
       " 'Derechos': 416,\n",
       " 'Durante': 417,\n",
       " 'muerte': 418,\n",
       " 'tarde': 419,\n",
       " 'haya': 420,\n",
       " 'Oficina': 421,\n",
       " 'Ley': 422,\n",
       " 'reunión': 423,\n",
       " 'guerra': 424,\n",
       " 'gestión': 425,\n",
       " 'L': 426,\n",
       " 'va': 427,\n",
       " 'cargo': 428,\n",
       " 'persona': 429,\n",
       " 'if': 430,\n",
       " 'dice': 431,\n",
       " 'organización': 432,\n",
       " 'Desde': 433,\n",
       " 'siguientes': 434,\n",
       " 'semana': 435,\n",
       " 'importancia': 436,\n",
       " 'falta': 437,\n",
       " 'ayuda': 438,\n",
       " 'últimos': 439,\n",
       " 'calidad': 440,\n",
       " 'seis': 441,\n",
       " 'violencia': 442,\n",
       " 'euros': 443,\n",
       " 'orden': 444,\n",
       " 'ver': 445,\n",
       " 'veces': 446,\n",
       " 'S': 447,\n",
       " 'edad': 448,\n",
       " 'medida': 449,\n",
       " 'especialmente': 450,\n",
       " 'instituciones': 451,\n",
       " 'papel': 452,\n",
       " 'necesario': 453,\n",
       " 'crisis': 454,\n",
       " 'esas': 455,\n",
       " 'temporada': 456,\n",
       " 'cuales': 457,\n",
       " 'necesidad': 458,\n",
       " 'nuevas': 459,\n",
       " 'proyectos': 460,\n",
       " 'director': 461,\n",
       " 'importantes': 462,\n",
       " 'problema': 463,\n",
       " 'dicho': 464,\n",
       " 'nuevos': 465,\n",
       " 'Desarrollo': 466,\n",
       " 'sur': 467,\n",
       " 'hombres': 468,\n",
       " 'objetivos': 469,\n",
       " 'español': 470,\n",
       " 'pesar': 471,\n",
       " 'labor': 472,\n",
       " 'algo': 473,\n",
       " 'Cuando': 474,\n",
       " 'sentido': 475,\n",
       " 'zonas': 476,\n",
       " 'cerca': 477,\n",
       " 'disposiciones': 478,\n",
       " 'quienes': 479,\n",
       " 'mundial': 480,\n",
       " 'Conferencia': 481,\n",
       " 'normas': 482,\n",
       " 'ex': 483,\n",
       " 'miembro': 484,\n",
       " 'plan': 485,\n",
       " 'acción': 486,\n",
       " 'propuesta': 487,\n",
       " 'pública': 488,\n",
       " 'principios': 489,\n",
       " 'plazo': 490,\n",
       " 'II': 491,\n",
       " 'Francia': 492,\n",
       " 'Entre': 493,\n",
       " 'mes': 494,\n",
       " 'Parlamento': 495,\n",
       " 'estudio': 496,\n",
       " 'creación': 497,\n",
       " 'lucha': 498,\n",
       " 'sigue': 499,\n",
       " 'muchas': 500,\n",
       " 'Carlos': 501,\n",
       " 'nuestro': 502,\n",
       " 'esfuerzos': 503,\n",
       " 'obras': 504,\n",
       " 'género': 505,\n",
       " 'Barcelona': 506,\n",
       " 'María': 507,\n",
       " 'Luis': 508,\n",
       " 'cuanto': 509,\n",
       " 'norte': 510,\n",
       " 'modo': 511,\n",
       " 'jóvenes': 512,\n",
       " 'resultado': 513,\n",
       " 'última': 514,\n",
       " 'Tras': 515,\n",
       " 'banda': 516,\n",
       " 'nuestra': 517,\n",
       " 'Ministerio': 518,\n",
       " 'cosas': 519,\n",
       " 'da': 520,\n",
       " 'Santa': 521,\n",
       " 'documento': 522,\n",
       " 'línea': 523,\n",
       " 'sociales': 524,\n",
       " 'llegar': 525,\n",
       " 'lado': 526,\n",
       " 'territorio': 527,\n",
       " 'informes': 528,\n",
       " 'económica': 529,\n",
       " 'dio': 530,\n",
       " 'anterior': 531,\n",
       " 'apartado': 532,\n",
       " 'construcción': 533,\n",
       " 'sería': 534,\n",
       " 'político': 535,\n",
       " 'éxito': 536,\n",
       " 'interés': 537,\n",
       " 'unas': 538,\n",
       " 'Tribunal': 539,\n",
       " 'valor': 540,\n",
       " 'local': 541,\n",
       " 'hombre': 542,\n",
       " 'conjunto': 543,\n",
       " 'alto': 544,\n",
       " 'igual': 545,\n",
       " 'estadounidense': 546,\n",
       " 'primeros': 547,\n",
       " 'libertad': 548,\n",
       " 'futuro': 549,\n",
       " 'principio': 550,\n",
       " 'oficial': 551,\n",
       " 'tierra': 552,\n",
       " 'Así': 553,\n",
       " 'gente': 554,\n",
       " 'examen': 555,\n",
       " 'origen': 556,\n",
       " 'mejorar': 557,\n",
       " 'Programa': 558,\n",
       " 'película': 559,\n",
       " 'América': 560,\n",
       " 'efectos': 561,\n",
       " 'elecciones': 562,\n",
       " 'nada': 563,\n",
       " 'favor': 564,\n",
       " 'formación': 565,\n",
       " 'elementos': 566,\n",
       " 'c': 567,\n",
       " 'fuerzas': 568,\n",
       " 'inglés': 569,\n",
       " 'lista': 570,\n",
       " 'civil': 571,\n",
       " 'Junta': 572,\n",
       " 'actividad': 573,\n",
       " 'ciudadanos': 574,\n",
       " 'Organización': 575,\n",
       " 'hijo': 576,\n",
       " 'partidos': 577,\n",
       " 'posibilidad': 578,\n",
       " 'sesión': 579,\n",
       " 'CE': 580,\n",
       " 'carrera': 581,\n",
       " 'común': 582,\n",
       " 'Lo': 583,\n",
       " 'función': 584,\n",
       " 'objeto': 585,\n",
       " 'especie': 586,\n",
       " 'ellas': 587,\n",
       " 'Humanos': 588,\n",
       " 'comunicación': 589,\n",
       " 'propio': 590,\n",
       " 'paso': 591,\n",
       " 'and': 592,\n",
       " 'música': 593,\n",
       " 'Especial': 594,\n",
       " 'locales': 595,\n",
       " 'título': 596,\n",
       " 'policía': 597,\n",
       " 'álbum': 598,\n",
       " 'necesidades': 599,\n",
       " 'Aunque': 600,\n",
       " 'noche': 601,\n",
       " 'operaciones': 602,\n",
       " 'estudios': 603,\n",
       " 'cómo': 604,\n",
       " 'seguir': 605,\n",
       " 'responsabilidad': 606,\n",
       " 'crecimiento': 607,\n",
       " 'padre': 608,\n",
       " 'empleo': 609,\n",
       " 'ningún': 610,\n",
       " 'obstante': 611,\n",
       " 'carácter': 612,\n",
       " 'dado': 613,\n",
       " 'aumento': 614,\n",
       " 'Comunidad': 615,\n",
       " 'n': 616,\n",
       " 'Centro': 617,\n",
       " 'Parte': 618,\n",
       " 'respuesta': 619,\n",
       " 'único': 620,\n",
       " 'posición': 621,\n",
       " 'próximo': 622,\n",
       " 'espacio': 623,\n",
       " 'mañana': 624,\n",
       " 'DE': 625,\n",
       " 'finales': 626,\n",
       " 'militar': 627,\n",
       " 'provincia': 628,\n",
       " 'ambos': 629,\n",
       " 'llegó': 630,\n",
       " 'realizar': 631,\n",
       " 'temas': 632,\n",
       " 'juego': 633,\n",
       " 'debate': 634,\n",
       " 'Argentina': 635,\n",
       " 'nunca': 636,\n",
       " 'demás': 637,\n",
       " 'dirección': 638,\n",
       " 'lograr': 639,\n",
       " 'metros': 640,\n",
       " 'económico': 641,\n",
       " 'práctica': 642,\n",
       " 'río': 643,\n",
       " 'presupuesto': 644,\n",
       " 'referencia': 645,\n",
       " 'evitar': 646,\n",
       " 'debería': 647,\n",
       " 'víctimas': 648,\n",
       " 's': 649,\n",
       " 'Después': 650,\n",
       " 'hora': 651,\n",
       " 'Secretaría': 652,\n",
       " 'economía': 653,\n",
       " 'sistemas': 654,\n",
       " 'York': 655,\n",
       " 'gracias': 656,\n",
       " 'Social': 657,\n",
       " 'fondos': 658,\n",
       " 'China': 659,\n",
       " 'hijos': 660,\n",
       " 'anexo': 661,\n",
       " 'primero': 662,\n",
       " 'acciones': 663,\n",
       " 'señaló': 664,\n",
       " 'buena': 665,\n",
       " 'Esto': 666,\n",
       " 'autor': 667,\n",
       " 'Instituto': 668,\n",
       " 'podrá': 669,\n",
       " 'régimen': 670,\n",
       " 'campaña': 671,\n",
       " 'B': 672,\n",
       " 'Reino': 673,\n",
       " 'conocido': 674,\n",
       " 'administración': 675,\n",
       " 'Antonio': 676,\n",
       " 'trabajadores': 677,\n",
       " 'todavía': 678,\n",
       " 'nota': 679,\n",
       " 'sean': 680,\n",
       " 'garantizar': 681,\n",
       " 'libre': 682,\n",
       " 'causa': 683,\n",
       " 'gastos': 684,\n",
       " 'presencia': 685,\n",
       " 'central': 686,\n",
       " 'pobreza': 687,\n",
       " 'menor': 688,\n",
       " 'dinero': 689,\n",
       " 'Trabajo': 690,\n",
       " 'ninguna': 691,\n",
       " 'época': 692,\n",
       " 'área': 693,\n",
       " 'Alemania': 694,\n",
       " 'transporte': 695,\n",
       " 'encuentran': 696,\n",
       " 'crear': 697,\n",
       " 'representantes': 698,\n",
       " 'pueda': 699,\n",
       " 'políticos': 700,\n",
       " 'hemos': 701,\n",
       " 'texto': 702,\n",
       " 'resto': 703,\n",
       " 'ingresos': 704,\n",
       " 'Francisco': 705,\n",
       " 'yo': 706,\n",
       " 'riesgo': 707,\n",
       " 'versión': 708,\n",
       " 'llamado': 709,\n",
       " 'Europeo': 710,\n",
       " 'aquí': 711,\n",
       " 'comercio': 712,\n",
       " 'término': 713,\n",
       " 'Partes': 714,\n",
       " 'informó': 715,\n",
       " 'campo': 716,\n",
       " 'efecto': 717,\n",
       " 'vista': 718,\n",
       " 'E': 719,\n",
       " 'funcionarios': 720,\n",
       " 'decisiones': 721,\n",
       " 'evaluación': 722,\n",
       " 'superior': 723,\n",
       " 'fuerza': 724,\n",
       " 'virtud': 725,\n",
       " 'recomendaciones': 726,\n",
       " 'estaban': 727,\n",
       " 'ejecución': 728,\n",
       " 'actualmente': 729,\n",
       " 'Manuel': 730,\n",
       " 'justicia': 731,\n",
       " 'organismos': 732,\n",
       " 'compañía': 733,\n",
       " 'cantidad': 734,\n",
       " 'declaración': 735,\n",
       " 'considera': 736,\n",
       " 'regionales': 737,\n",
       " 'calle': 738,\n",
       " 'televisión': 739,\n",
       " 'equipos': 740,\n",
       " 'alguna': 741,\n",
       " 'promover': 742,\n",
       " 'minutos': 743,\n",
       " 'Real': 744,\n",
       " 'parece': 745,\n",
       " 'cuerpo': 746,\n",
       " 'propia': 747,\n",
       " 'ámbito': 748,\n",
       " 'energía': 749,\n",
       " 'comenzó': 750,\n",
       " 'modelo': 751,\n",
       " 'reforma': 752,\n",
       " 'arreglo': 753,\n",
       " 'formas': 754,\n",
       " 'espera': 755,\n",
       " 'Partido': 756,\n",
       " 'establecer': 757,\n",
       " 'ministro': 758,\n",
       " 'éste': 759,\n",
       " 'mantener': 760,\n",
       " 'relaciones': 761,\n",
       " 'ambiente': 762,\n",
       " 'tenido': 763,\n",
       " 'Congreso': 764,\n",
       " 'opinión': 765,\n",
       " 'tercer': 766,\n",
       " 'Chile': 767,\n",
       " 'puedan': 768,\n",
       " 'diversos': 769,\n",
       " 'canción': 770,\n",
       " 'siete': 771,\n",
       " 'regional': 772,\n",
       " 'menores': 773,\n",
       " 'alrededor': 774,\n",
       " 'existe': 775,\n",
       " 'alta': 776,\n",
       " 'nuestros': 777,\n",
       " 'joven': 778,\n",
       " 'acerca': 779,\n",
       " 'Sur': 780,\n",
       " 'cuyo': 781,\n",
       " 'legislación': 782,\n",
       " 'conocer': 783,\n",
       " 'gobiernos': 784,\n",
       " 'van': 785,\n",
       " 'cuya': 786,\n",
       " 'Guerra': 787,\n",
       " 'financiación': 788,\n",
       " 'club': 789,\n",
       " 'funciones': 790,\n",
       " 'serán': 791,\n",
       " 'tenemos': 792,\n",
       " 'fuentes': 793,\n",
       " 'Departamento': 794,\n",
       " 'compromiso': 795,\n",
       " 'realidad': 796,\n",
       " 'Ciudad': 797,\n",
       " 'Gran': 798,\n",
       " 'localidad': 799,\n",
       " 'figura': 800,\n",
       " 'poner': 801,\n",
       " 'consecuencia': 802,\n",
       " 'técnica': 803,\n",
       " 'mandato': 804,\n",
       " 'Italia': 805,\n",
       " 'Norte': 806,\n",
       " 'procedimiento': 807,\n",
       " 'Hay': 808,\n",
       " 'relativas': 809,\n",
       " 'UE': 810,\n",
       " 'mantenimiento': 811,\n",
       " 'red': 812,\n",
       " 'entrada': 813,\n",
       " 'familias': 814,\n",
       " 'actos': 815,\n",
       " 'artículos': 816,\n",
       " 'principalmente': 817,\n",
       " 'conflicto': 818,\n",
       " 'interior': 819,\n",
       " 'prensa': 820,\n",
       " 'Israel': 821,\n",
       " 'tales': 822,\n",
       " 'dicha': 823,\n",
       " 'mejores': 824,\n",
       " 'documentos': 825,\n",
       " 'puestos': 826,\n",
       " 'marcha': 827,\n",
       " 'media': 828,\n",
       " 'Miembros': 829,\n",
       " 'niño': 830,\n",
       " 'experiencia': 831,\n",
       " 'hechos': 832,\n",
       " 'representante': 833,\n",
       " 'defensa': 834,\n",
       " 'Unido': 835,\n",
       " 'centros': 836,\n",
       " 'discriminación': 837,\n",
       " 'sede': 838,\n",
       " 'presentación': 839,\n",
       " 'madre': 840,\n",
       " 'terrorismo': 841,\n",
       " 'diversas': 842,\n",
       " 'Policía': 843,\n",
       " 'propuestas': 844,\n",
       " 'sitio': 845,\n",
       " 'pueblos': 846,\n",
       " 'Justicia': 847,\n",
       " 'nosotros': 848,\n",
       " 'trabajos': 849,\n",
       " 'libro': 850,\n",
       " 'oportunidad': 851,\n",
       " 'prácticas': 852,\n",
       " 'tomar': 853,\n",
       " 'análisis': 854,\n",
       " 'permite': 855,\n",
       " 'promoción': 856,\n",
       " 'muestra': 857,\n",
       " 'cambios': 858,\n",
       " 'Tratado': 859,\n",
       " 'fuerte': 860,\n",
       " 'cumplir': 861,\n",
       " 'estrategia': 862,\n",
       " 'ubicado': 863,\n",
       " 'colaboración': 864,\n",
       " 'Directiva': 865,\n",
       " 'López': 866,\n",
       " 'delegación': 867,\n",
       " 'estuvo': 868,\n",
       " 'ocho': 869,\n",
       " 'militares': 870,\n",
       " 'órganos': 871,\n",
       " 'mano': 872,\n",
       " 'única': 873,\n",
       " 'pesos': 874,\n",
       " 'categoría': 875,\n",
       " 'incluye': 876,\n",
       " 'km': 877,\n",
       " 'in': 878,\n",
       " 'española': 879,\n",
       " 'Pedro': 880,\n",
       " 'solución': 881,\n",
       " 'mar': 882,\n",
       " 'generales': 883,\n",
       " 'rey': 884,\n",
       " 'cumplimiento': 885,\n",
       " 'Fondo': 886,\n",
       " 'PP': 887,\n",
       " 'Sevilla': 888,\n",
       " 'Rusia': 889,\n",
       " 'color': 890,\n",
       " 'jefe': 891,\n",
       " 'isla': 892,\n",
       " 'llevar': 893,\n",
       " 'francés': 894,\n",
       " 'León': 895,\n",
       " 'producto': 896,\n",
       " 'estamos': 897,\n",
       " 'obtener': 898,\n",
       " 'contexto': 899,\n",
       " 'niveles': 900,\n",
       " 'fondo': 901,\n",
       " 'Brasil': 902,\n",
       " 'tecnología': 903,\n",
       " 'aspectos': 904,\n",
       " 'iniciativa': 905,\n",
       " 'tendrá': 906,\n",
       " 'aseguró': 907,\n",
       " 'Liga': 908,\n",
       " 'padres': 909,\n",
       " 'precio': 910,\n",
       " 'buen': 911,\n",
       " 'ejercicio': 912,\n",
       " 'Miguel': 913,\n",
       " 'cultura': 914,\n",
       " 'García': 915,\n",
       " 'procedimientos': 916,\n",
       " 'venta': 917,\n",
       " 'competencia': 918,\n",
       " 'agentes': 919,\n",
       " 'camino': 920,\n",
       " 'he': 921,\n",
       " 'trabajar': 922,\n",
       " 'igualdad': 923,\n",
       " 'material': 924,\n",
       " 'presentó': 925,\n",
       " 'explicó': 926,\n",
       " 'Japón': 927,\n",
       " 'pertinentes': 928,\n",
       " 'mayores': 929,\n",
       " 'superficie': 930,\n",
       " 'luz': 931,\n",
       " 'alcanzar': 932,\n",
       " 'profesional': 933,\n",
       " 'coordinación': 934,\n",
       " 'recibió': 935,\n",
       " 'lugares': 936,\n",
       " 'comunidades': 937,\n",
       " 'instrumentos': 938,\n",
       " 'establecimiento': 939,\n",
       " 'líder': 940,\n",
       " 'reducción': 941,\n",
       " 'difícil': 942,\n",
       " 'especiales': 943,\n",
       " 'palabras': 944,\n",
       " 'movimiento': 945,\n",
       " 'real': 946,\n",
       " 'decidió': 947,\n",
       " 'distrito': 948,\n",
       " 'razón': 949,\n",
       " 'leyes': 950,\n",
       " 'acuerdos': 951,\n",
       " 'deberá': 952,\n",
       " 'públicos': 953,\n",
       " 'fútbol': 954,\n",
       " 'Federal': 955,\n",
       " 'Señor': 956,\n",
       " 'algún': 957,\n",
       " 'conflictos': 958,\n",
       " 'presentado': 959,\n",
       " 'División': 960,\n",
       " 'relativa': 961,\n",
       " 'establecido': 962,\n",
       " 'reducir': 963,\n",
       " 'disco': 964,\n",
       " 'fines': 965,\n",
       " 'oficiales': 966,\n",
       " 'expertos': 967,\n",
       " 'máximo': 968,\n",
       " 'diez': 969,\n",
       " 'precios': 970,\n",
       " 'Asimismo': 971,\n",
       " 'disposición': 972,\n",
       " 'claro': 973,\n",
       " 'autoridad': 974,\n",
       " 'mitad': 975,\n",
       " 'u': 976,\n",
       " 'reuniones': 977,\n",
       " 'idea': 978,\n",
       " 'participar': 979,\n",
       " 'saber': 980,\n",
       " 'Constitución': 981,\n",
       " 'edición': 982,\n",
       " 'refiere': 983,\n",
       " 'hayan': 984,\n",
       " 'ciudades': 985,\n",
       " 'Asuntos': 986,\n",
       " 'estilo': 987,\n",
       " 'lleva': 988,\n",
       " 'dispuesto': 989,\n",
       " 'curso': 990,\n",
       " 'bienes': 991,\n",
       " 'imagen': 992,\n",
       " 'Costa': 993,\n",
       " 'gubernamentales': 994,\n",
       " 'distintos': 995,\n",
       " 'sectores': 996,\n",
       " 'realizado': 997,\n",
       " 'continuación': 998,\n",
       " 'pruebas': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words and possition (val)\n",
    "word2Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Padding the sentences \n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the word according to the caseLookup ( numeric, mainly_numeric, allLower, allUpper, initialUpper, contains_digit) \n",
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    #Get number of digits in word\n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create words embeding matrices to padding\n",
    "def createMatrices(sentences, word2Idx, label2Idx, case2Idx,char2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word,char,label in sentence:  \n",
    "            wordCount += 1\n",
    "            # if the word is in the list of words to index, then index it (verify with the lower cased word)\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]                 \n",
    "            else: # else tag it as unknown\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                charIdx.append(char2Idx[x])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx)) #Call getCasing\n",
    "            charIndices.append(charIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the train/dev/test set and convert them to embedings\n",
    "\n",
    "train_set = padding(createMatrices(trainSentences,word2Idx,  label2Idx, case2Idx,char2Idx))\n",
    "dev_set = padding(createMatrices(devSentences,word2Idx, label2Idx, case2Idx,char2Idx))\n",
    "test_set = padding(createMatrices(testSentences, word2Idx, label2Idx, case2Idx,char2Idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Melbourne', ['M', 'e', 'l', 'b', 'o', 'u', 'r', 'n', 'e'], 'B-LOC'],\n",
       " ['(', ['('], 'O'],\n",
       " ['Australia', ['A', 'u', 's', 't', 'r', 'a', 'l', 'i', 'a'], 'B-LOC'],\n",
       " [')', [')'], 'O'],\n",
       " [',', [','], 'O'],\n",
       " ['25', ['2', '5'], 'O'],\n",
       " ['may', ['m', 'a', 'y'], 'O'],\n",
       " ['(', ['('], 'O'],\n",
       " ['EFE', ['E', 'F', 'E'], 'B-ORG'],\n",
       " [')', [')'], 'O'],\n",
       " ['.', ['.'], 'O']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11707, 1, 1439, 1, 1, 1, 16426, 1, 6411, 1, 1],\n",
       " [3, 4, 3, 4, 4, 0, 1, 4, 2, 4, 4],\n",
       " array([[ 51,  17,  24,  14,  27,  33,  30,  26,  17,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [103,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 39,  33,  31,  32,  30,  13,  24,  21,  13,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [104,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [100,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  5,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 25,  13,  37,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [103,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 43,  44,  43,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [104,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 99,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]),\n",
       " [4, 7, 4, 7, 7, 7, 7, 7, 0, 7, 7]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the words and labels to index as dict types\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "#***************************************PATH***********************\n",
    "np.save(\"model_data/idx2Label.npy\",idx2Label)\n",
    "np.save(\"model_data/word2Idx.npy\",word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch for each set (later we will create mini-batch)\n",
    "def createBatches(data):\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(len(i[0]))\n",
    "    l = set(l)\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if len(batch[0]) == i:\n",
    "                batches.append(batch)\n",
    "                z += 1\n",
    "        batch_len.append(z)\n",
    "    return batches,batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch,train_batch_len = createBatches(train_set)\n",
    "dev_batch,dev_batch_len = createBatches(dev_set)\n",
    "test_batch,test_batch_len = createBatches(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_batch_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with Tensorflow. Remember that tf first construct a graph, and then run it. tf automatically determines the best contruction taking into consideration each node requirements.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor for the inputs\n",
    "words_input = Input(shape=(None,),dtype='int32',name='words_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Users\\Fede\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor of the embeddings using the words embeddings and feeding with the words_input tensor\n",
    "words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor of casing input\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a tensor of the casing using the words embeddings and feeding with the casing_input tensor\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More tensors for the model...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_input=Input(shape=(None,52,),name='char_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Users\\Fede\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Establish the dropout (neurons?)\n",
    "dropout= Dropout(0.5)(embed_char_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max pool of the convolutional\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattern  layer for the CNN, it is requered to be flattern for the CNN\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = concatenate([words, casing,char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\n",
    "output = TimeDistributed(Dense(len(label2Idx), activation='softmax'))(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model. Inlcudes a Summary of the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 30) 4260        char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 52, 30) 0           char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 52, 30) 2730        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 1, 30)  0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, None, 30)     0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    16445700    words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 30)     0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 338)    0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 400)    862400      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, None, 9)      3609        bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 17,318,763\n",
      "Trainable params: 872,999\n",
      "Non-trainable params: 16,445,764\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[words_input, casing_input,character_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of epochs\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatches\n",
    "def iterate_minibatches(dataset,batch_len): \n",
    "    start = 0\n",
    "    for i in batch_len:\n",
    "        tokens = []\n",
    "        caseing = []\n",
    "        char = []\n",
    "        labels = []\n",
    "        data = dataset[start:i]\n",
    "        start = i\n",
    "        for dt in data:\n",
    "            t,c,ch,l = dt\n",
    "            l = np.expand_dims(l,-1)\n",
    "            tokens.append(t)\n",
    "            caseing.append(c)\n",
    "            char.append(ch)\n",
    "            labels.append(l)\n",
    "        yield np.asarray(labels),np.asarray(tokens),np.asarray(caseing),np.asarray(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/150\n",
      "WARNING:tensorflow:From E:\\Users\\Fede\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "109/109 [==============================] - 46s 420ms/step\n",
      " \n",
      "Epoch 1/150\n",
      "109/109 [==============================] - 43s 399ms/step\n",
      " \n",
      "Epoch 2/150\n",
      "109/109 [==============================] - 44s 406ms/step\n",
      " \n",
      "Epoch 3/150\n",
      "109/109 [==============================] - 46s 421ms/step\n",
      " \n",
      "Epoch 4/150\n",
      "109/109 [==============================] - 46s 419ms/step\n",
      " \n",
      "Epoch 5/150\n",
      "109/109 [==============================] - 46s 418ms/step\n",
      " \n",
      "Epoch 6/150\n",
      "109/109 [==============================] - 46s 424ms/step\n",
      " \n",
      "Epoch 7/150\n",
      "109/109 [==============================] - 46s 423ms/step\n",
      " \n",
      "Epoch 8/150\n",
      "109/109 [==============================] - 47s 427ms/step\n",
      " \n",
      "Epoch 9/150\n",
      "109/109 [==============================] - 47s 427ms/step\n",
      " \n",
      "Epoch 10/150\n",
      "109/109 [==============================] - 47s 433ms/step\n",
      " \n",
      "Epoch 11/150\n",
      "109/109 [==============================] - 47s 431ms/step\n",
      " \n",
      "Epoch 12/150\n",
      "109/109 [==============================] - 47s 428ms/step\n",
      " \n",
      "Epoch 13/150\n",
      "109/109 [==============================] - 47s 431ms/step\n",
      " \n",
      "Epoch 14/150\n",
      "109/109 [==============================] - 48s 441ms/step\n",
      " \n",
      "Epoch 15/150\n",
      "109/109 [==============================] - 47s 431ms/step\n",
      " \n",
      "Epoch 16/150\n",
      "109/109 [==============================] - 47s 431ms/step\n",
      " \n",
      "Epoch 17/150\n",
      "109/109 [==============================] - 47s 430ms/step\n",
      " \n",
      "Epoch 18/150\n",
      "109/109 [==============================] - 47s 433ms/step\n",
      " \n",
      "Epoch 19/150\n",
      "109/109 [==============================] - 47s 432ms/step\n",
      " \n",
      "Epoch 20/150\n",
      "109/109 [==============================] - 47s 434ms/step\n",
      " \n",
      "Epoch 21/150\n",
      "109/109 [==============================] - 47s 431ms/step\n",
      " \n",
      "Epoch 22/150\n",
      "109/109 [==============================] - 48s 436ms/step\n",
      " \n",
      "Epoch 23/150\n",
      "109/109 [==============================] - 47s 433ms/step\n",
      " \n",
      "Epoch 24/150\n",
      "109/109 [==============================] - 47s 434ms/step\n",
      " \n",
      "Epoch 25/150\n",
      "109/109 [==============================] - 48s 439ms/step\n",
      " \n",
      "Epoch 26/150\n",
      "109/109 [==============================] - 47s 431ms/step\n",
      " \n",
      "Epoch 27/150\n",
      "109/109 [==============================] - 47s 431ms/step\n",
      " \n",
      "Epoch 28/150\n",
      "109/109 [==============================] - 50s 457ms/step\n",
      " \n",
      "Epoch 29/150\n",
      "109/109 [==============================] - 49s 449ms/step\n",
      " \n",
      "Epoch 30/150\n",
      "109/109 [==============================] - 48s 436ms/step\n",
      " \n",
      "Epoch 31/150\n",
      "109/109 [==============================] - 48s 437ms/step\n",
      " \n",
      "Epoch 32/150\n",
      "109/109 [==============================] - 47s 434ms/step\n",
      " \n",
      "Epoch 33/150\n",
      "109/109 [==============================] - 49s 445ms/step\n",
      " \n",
      "Epoch 34/150\n",
      "109/109 [==============================] - 47s 431ms/step\n",
      " \n",
      "Epoch 35/150\n",
      "109/109 [==============================] - 48s 440ms/step\n",
      " \n",
      "Epoch 36/150\n",
      "109/109 [==============================] - 48s 438ms/step\n",
      " \n",
      "Epoch 37/150\n",
      "109/109 [==============================] - 47s 432ms/step\n",
      " \n",
      "Epoch 38/150\n",
      "109/109 [==============================] - 47s 435ms/step\n",
      " \n",
      "Epoch 39/150\n",
      "109/109 [==============================] - 48s 437ms/step\n",
      " \n",
      "Epoch 40/150\n",
      "109/109 [==============================] - 48s 438ms/step\n",
      " \n",
      "Epoch 41/150\n",
      "109/109 [==============================] - 47s 434ms/step\n",
      " \n",
      "Epoch 42/150\n",
      "109/109 [==============================] - 48s 442ms/step\n",
      " \n",
      "Epoch 43/150\n",
      "109/109 [==============================] - 48s 437ms/step\n",
      " \n",
      "Epoch 44/150\n",
      "109/109 [==============================] - 48s 436ms/step\n",
      " \n",
      "Epoch 45/150\n",
      "109/109 [==============================] - 48s 443ms/step\n",
      " \n",
      "Epoch 46/150\n",
      "109/109 [==============================] - 48s 436ms/step\n",
      " \n",
      "Epoch 47/150\n",
      "109/109 [==============================] - 47s 433ms/step\n",
      " \n",
      "Epoch 48/150\n",
      "109/109 [==============================] - 48s 438ms/step\n",
      " \n",
      "Epoch 49/150\n",
      "109/109 [==============================] - 48s 437ms/step\n",
      " \n",
      "Epoch 50/150\n",
      "109/109 [==============================] - 48s 440ms/step\n",
      " \n",
      "Epoch 51/150\n",
      "109/109 [==============================] - 48s 444ms/step\n",
      " \n",
      "Epoch 52/150\n",
      "109/109 [==============================] - 48s 442ms/step\n",
      " \n",
      "Epoch 53/150\n",
      "109/109 [==============================] - 48s 445ms/step\n",
      " \n",
      "Epoch 54/150\n",
      "109/109 [==============================] - 51s 464ms/step\n",
      " \n",
      "Epoch 55/150\n",
      "109/109 [==============================] - 49s 452ms/step\n",
      " \n",
      "Epoch 56/150\n",
      "109/109 [==============================] - 48s 441ms/step\n",
      " \n",
      "Epoch 57/150\n",
      "109/109 [==============================] - 48s 437ms/step\n",
      " \n",
      "Epoch 58/150\n",
      "109/109 [==============================] - 48s 441ms/step\n",
      " \n",
      "Epoch 59/150\n",
      "109/109 [==============================] - 48s 439ms/step\n",
      " \n",
      "Epoch 60/150\n",
      "109/109 [==============================] - 48s 442ms/step\n",
      " \n",
      "Epoch 61/150\n",
      "109/109 [==============================] - 48s 440ms/step\n",
      " \n",
      "Epoch 62/150\n",
      "109/109 [==============================] - 49s 447ms/step\n",
      " \n",
      "Epoch 63/150\n",
      "109/109 [==============================] - 48s 437ms/step\n",
      " \n",
      "Epoch 64/150\n",
      "109/109 [==============================] - 48s 439ms/step\n",
      " \n",
      "Epoch 65/150\n",
      "109/109 [==============================] - 48s 443ms/step\n",
      " \n",
      "Epoch 66/150\n",
      "109/109 [==============================] - 49s 445ms/step\n",
      " \n",
      "Epoch 67/150\n",
      "109/109 [==============================] - 49s 451ms/step\n",
      " \n",
      "Epoch 68/150\n",
      "109/109 [==============================] - 48s 438ms/step\n",
      " \n",
      "Epoch 69/150\n",
      "109/109 [==============================] - 48s 442ms/step\n",
      " \n",
      "Epoch 70/150\n",
      "109/109 [==============================] - 49s 451ms/step\n",
      " \n",
      "Epoch 71/150\n",
      "109/109 [==============================] - 48s 445ms/step\n",
      " \n",
      "Epoch 72/150\n",
      "109/109 [==============================] - 50s 455ms/step\n",
      " \n",
      "Epoch 73/150\n",
      "109/109 [==============================] - 48s 442ms/step\n",
      " \n",
      "Epoch 74/150\n",
      "109/109 [==============================] - 49s 448ms/step\n",
      " \n",
      "Epoch 75/150\n",
      "109/109 [==============================] - 53s 483ms/step\n",
      " \n",
      "Epoch 76/150\n",
      "109/109 [==============================] - 53s 490ms/step\n",
      " \n",
      "Epoch 77/150\n",
      "109/109 [==============================] - 53s 482ms/step\n",
      " \n",
      "Epoch 78/150\n",
      "109/109 [==============================] - 50s 459ms/step\n",
      " \n",
      "Epoch 79/150\n",
      "109/109 [==============================] - 51s 465ms/step\n",
      " \n",
      "Epoch 80/150\n",
      "109/109 [==============================] - 51s 465ms/step\n",
      " \n",
      "Epoch 81/150\n",
      "109/109 [==============================] - 51s 464ms/step\n",
      " \n",
      "Epoch 82/150\n",
      "109/109 [==============================] - 54s 496ms/step\n",
      " \n",
      "Epoch 83/150\n",
      "109/109 [==============================] - 53s 484ms/step\n",
      " \n",
      "Epoch 84/150\n",
      "109/109 [==============================] - 52s 474ms/step\n",
      " \n",
      "Epoch 85/150\n",
      "109/109 [==============================] - 54s 499ms/step\n",
      " \n",
      "Epoch 86/150\n",
      "109/109 [==============================] - 49s 447ms/step\n",
      " \n",
      "Epoch 87/150\n",
      "109/109 [==============================] - 48s 444ms/step\n",
      " \n",
      "Epoch 88/150\n",
      "109/109 [==============================] - 49s 446ms/step\n",
      " \n",
      "Epoch 89/150\n",
      "109/109 [==============================] - 48s 441ms/step\n",
      " \n",
      "Epoch 90/150\n",
      "109/109 [==============================] - 48s 442ms/step\n",
      " \n",
      "Epoch 91/150\n",
      "109/109 [==============================] - 49s 446ms/step\n",
      " \n",
      "Epoch 92/150\n",
      "109/109 [==============================] - 49s 445ms/step\n",
      " \n",
      "Epoch 93/150\n",
      "109/109 [==============================] - 48s 440ms/step\n",
      " \n",
      "Epoch 94/150\n",
      "109/109 [==============================] - 48s 445ms/step\n",
      " \n",
      "Epoch 95/150\n",
      "109/109 [==============================] - 49s 446ms/step\n",
      " \n",
      "Epoch 96/150\n",
      "109/109 [==============================] - 48s 436ms/step\n",
      " \n",
      "Epoch 97/150\n",
      "109/109 [==============================] - 48s 443ms/step\n",
      " \n",
      "Epoch 98/150\n",
      "109/109 [==============================] - 48s 439ms/step\n",
      " \n",
      "Epoch 99/150\n",
      "109/109 [==============================] - 48s 441ms/step\n",
      " \n",
      "Epoch 100/150\n",
      "109/109 [==============================] - 48s 439ms/step\n",
      " \n",
      "Epoch 101/150\n",
      "109/109 [==============================] - 48s 440ms/step\n",
      " \n",
      "Epoch 102/150\n",
      "109/109 [==============================] - 48s 441ms/step\n",
      " \n",
      "Epoch 103/150\n",
      "109/109 [==============================] - 48s 443ms/step\n",
      " \n",
      "Epoch 104/150\n",
      "109/109 [==============================] - 48s 444ms/step\n",
      " \n",
      "Epoch 105/150\n",
      "109/109 [==============================] - 49s 449ms/step\n",
      " \n",
      "Epoch 106/150\n",
      "109/109 [==============================] - 48s 445ms/step\n",
      " \n",
      "Epoch 107/150\n",
      "109/109 [==============================] - 49s 447ms/step\n",
      " \n",
      "Epoch 108/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 47s 436ms/step\n",
      " \n",
      "Epoch 109/150\n",
      "109/109 [==============================] - 48s 442ms/step\n",
      " \n",
      "Epoch 110/150\n",
      "109/109 [==============================] - 48s 437ms/step\n",
      " \n",
      "Epoch 111/150\n",
      "109/109 [==============================] - 48s 439ms/step\n",
      " \n",
      "Epoch 112/150\n",
      "109/109 [==============================] - 48s 445ms/step\n",
      " \n",
      "Epoch 113/150\n",
      "109/109 [==============================] - 48s 440ms/step\n",
      " \n",
      "Epoch 114/150\n",
      "109/109 [==============================] - 48s 438ms/step\n",
      " \n",
      "Epoch 115/150\n",
      "109/109 [==============================] - 48s 444ms/step\n",
      " \n",
      "Epoch 116/150\n",
      "109/109 [==============================] - 49s 447ms/step\n",
      " \n",
      "Epoch 117/150\n",
      "109/109 [==============================] - 48s 439ms/step\n",
      " \n",
      "Epoch 118/150\n",
      "109/109 [==============================] - 49s 447ms/step\n",
      " \n",
      "Epoch 119/150\n",
      "109/109 [==============================] - 48s 444ms/step\n",
      " \n",
      "Epoch 120/150\n",
      "109/109 [==============================] - 48s 442ms/step\n",
      " \n",
      "Epoch 121/150\n",
      "109/109 [==============================] - 48s 436ms/step\n",
      " \n",
      "Epoch 122/150\n",
      "109/109 [==============================] - 48s 437ms/step\n",
      " \n",
      "Epoch 123/150\n",
      "109/109 [==============================] - 48s 439ms/step\n",
      " \n",
      "Epoch 124/150\n",
      "109/109 [==============================] - 48s 436ms/step\n",
      " \n",
      "Epoch 125/150\n",
      "109/109 [==============================] - 48s 439ms/step\n",
      " \n",
      "Epoch 126/150\n",
      "109/109 [==============================] - 48s 445ms/step\n",
      " \n",
      "Epoch 127/150\n",
      "109/109 [==============================] - 48s 442ms/step\n",
      " \n",
      "Epoch 128/150\n",
      "109/109 [==============================] - 48s 440ms/step\n",
      " \n",
      "Epoch 129/150\n",
      "109/109 [==============================] - 49s 445ms/step\n",
      " \n",
      "Epoch 130/150\n",
      "109/109 [==============================] - 49s 446ms/step\n",
      " \n",
      "Epoch 131/150\n",
      "109/109 [==============================] - 48s 441ms/step\n",
      " \n",
      "Epoch 132/150\n",
      "109/109 [==============================] - 48s 445ms/step\n",
      " \n",
      "Epoch 133/150\n",
      "109/109 [==============================] - 48s 444ms/step\n",
      " \n",
      "Epoch 134/150\n",
      "109/109 [==============================] - 48s 444ms/step\n",
      " \n",
      "Epoch 135/150\n",
      "109/109 [==============================] - 49s 446ms/step\n",
      " \n",
      "Epoch 136/150\n",
      "109/109 [==============================] - 48s 445ms/step\n",
      " \n",
      "Epoch 137/150\n",
      "109/109 [==============================] - 49s 445ms/step\n",
      " \n",
      "Epoch 138/150\n",
      "109/109 [==============================] - 49s 446ms/step\n",
      " \n",
      "Epoch 139/150\n",
      "109/109 [==============================] - 48s 441ms/step\n",
      " \n",
      "Epoch 140/150\n",
      "109/109 [==============================] - 48s 443ms/step\n",
      " \n",
      "Epoch 141/150\n",
      "109/109 [==============================] - 49s 446ms/step\n",
      " \n",
      "Epoch 142/150\n",
      "109/109 [==============================] - 48s 441ms/step\n",
      " \n",
      "Epoch 143/150\n",
      "109/109 [==============================] - 48s 439ms/step\n",
      " \n",
      "Epoch 144/150\n",
      "109/109 [==============================] - 49s 447ms/step\n",
      " \n",
      "Epoch 145/150\n",
      "109/109 [==============================] - 49s 450ms/step\n",
      " \n",
      "Epoch 146/150\n",
      "109/109 [==============================] - 49s 452ms/step\n",
      " \n",
      "Epoch 147/150\n",
      "109/109 [==============================] - 48s 443ms/step\n",
      " \n",
      "Epoch 148/150\n",
      "109/109 [==============================] - 48s 444ms/step\n",
      " \n",
      "Epoch 149/150\n",
      "109/109 [==============================] - 48s 437ms/step\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Training of n epochs \n",
    "for epoch in range(epochs):    \n",
    "    print(\"Epoch %d/%d\"%(epoch,epochs))\n",
    "    a = Progbar(len(train_batch_len))\n",
    "    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
    "        labels, tokens, casing,char = batch       \n",
    "        model.train_on_batch([tokens, casing,char], labels)\n",
    "        a.update(i)\n",
    "    a.update(i+1)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save(\"model_data/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating model accurracy. Using F1, precision  and recal for Dev and Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    b.update(i+1)\n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to compute the accruarcy. Call predict_labels to get the labels for the dataset\n",
    "def compute_f1(predictions, correct, idx2Label): \n",
    "    label_pred = []    \n",
    "    for sentence in predictions:\n",
    "        label_pred.append([idx2Label[element] for element in sentence])\n",
    "        \n",
    "    label_correct = []    \n",
    "    for sentence in correct:\n",
    "        label_correct.append([idx2Label[element] for element in sentence])\n",
    "            \n",
    "    \n",
    "    #print label_pred\n",
    "    #print label_correct\n",
    "    \n",
    "    prec = compute_precision(label_pred, label_correct)\n",
    "    rec = compute_precision(label_correct, label_pred)\n",
    "    \n",
    "    f1 = 0\n",
    "    if (rec+prec) > 0:\n",
    "        f1 = 2.0 * prec * rec / (prec + rec);\n",
    "        \n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(guessed_sentences, correct_sentences):\n",
    "    assert(len(guessed_sentences) == len(correct_sentences))\n",
    "    correctCount = 0\n",
    "    count = 0\n",
    "    \n",
    "    \n",
    "    for sentenceIdx in range(len(guessed_sentences)):\n",
    "        guessed = guessed_sentences[sentenceIdx]\n",
    "        correct = correct_sentences[sentenceIdx]\n",
    "        assert(len(guessed) == len(correct))\n",
    "        idx = 0\n",
    "        while idx < len(guessed):\n",
    "            if guessed[idx][0] == 'B': #A new chunk starts\n",
    "                count += 1\n",
    "                \n",
    "                if guessed[idx] == correct[idx]:\n",
    "                    idx += 1\n",
    "                    correctlyFound = True\n",
    "                    \n",
    "                    while idx < len(guessed) and guessed[idx][0] == 'I': #Scan until it no longer starts with I\n",
    "                        if guessed[idx] != correct[idx]:\n",
    "                            correctlyFound = False\n",
    "                        \n",
    "                        idx += 1\n",
    "                    \n",
    "                    if idx < len(guessed):\n",
    "                        if correct[idx][0] == 'I': #The chunk in correct was longer\n",
    "                            correctlyFound = False\n",
    "                        \n",
    "                    \n",
    "                    if correctlyFound:\n",
    "                        correctCount += 1\n",
    "                else:\n",
    "                    idx += 1\n",
    "            else:  \n",
    "                idx += 1\n",
    "    \n",
    "    precision = 0\n",
    "    if count > 0:    \n",
    "        precision = float(correctCount) / count\n",
    "        \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915/1915 [==============================] - 9s 5ms/step\n",
      "Dev-Data: Prec: 0.811, Rec: 0.813, F1: 0.812\n"
     ]
    }
   ],
   "source": [
    "#   Performance on dev dataset        \n",
    "predLabels, correctLabels = tag_dataset(dev_batch)        \n",
    "pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, idx2Label)\n",
    "print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1517/1517 [==============================] - 8s 5ms/step\n",
      "Test-Data: Prec: 0.849, Rec: 0.853, F1: 0.851\n"
     ]
    }
   ],
   "source": [
    "#   Performance on test dataset       \n",
    "predLabels, correctLabels = tag_dataset(test_batch)        \n",
    "pre_test, rec_test, f1_test= compute_f1(predLabels, correctLabels, idx2Label)\n",
    "print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_test, rec_test, f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining class for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import word_tokenize\n",
    "\n",
    "class Parser:\n",
    "\n",
    "    def __init__(self):\n",
    "        # ::Hard coded char lookup ::\n",
    "        self.char2Idx = {\"PADDING\":0, \"UNKNOWN\":1}\n",
    "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
    "            self.char2Idx[c] = len(self.char2Idx)\n",
    "        # :: Hard coded case lookup ::\n",
    "        self.case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "\n",
    "    def load_models(self, loc=None):\n",
    "        if not loc:\n",
    "            loc = os.path.join(os.path.expanduser('~'), '.ner_model')\n",
    "        self.model = load_model(os.path.join(loc,\"model.h5\"))\n",
    "        # loading word2Idx\n",
    "        self.word2Idx = np.load(os.path.join(loc,\"word2Idx.npy\")).item()\n",
    "        # loading idx2Label\n",
    "        self.idx2Label = np.load(os.path.join(loc,\"idx2Label.npy\")).item()\n",
    "\n",
    "    def getCasing(self,word, caseLookup):   \n",
    "        casing = 'other'\n",
    "        \n",
    "        numDigits = 0\n",
    "        for char in word:\n",
    "            if char.isdigit():\n",
    "                numDigits += 1\n",
    "                \n",
    "        digitFraction = numDigits / float(len(word))\n",
    "        \n",
    "        if word.isdigit(): #Is a digit\n",
    "            casing = 'numeric'\n",
    "        elif digitFraction > 0.5:\n",
    "            casing = 'mainly_numeric'\n",
    "        elif word.islower(): #All lower case\n",
    "            casing = 'allLower'\n",
    "        elif word.isupper(): #All upper case\n",
    "            casing = 'allUpper'\n",
    "        elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "            casing = 'initialUpper'\n",
    "        elif numDigits > 0:\n",
    "            casing = 'contains_digit'  \n",
    "        return caseLookup[casing]\n",
    "\n",
    "    def createTensor(self,sentence, word2Idx,case2Idx,char2Idx):\n",
    "        unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    \n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "            \n",
    "        for word,char in sentence:  \n",
    "            word = str(word)\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                if x in char2Idx.keys():\n",
    "                    charIdx.append(char2Idx[x])\n",
    "                else:\n",
    "                    charIdx.append(char2Idx['UNKNOWN'])   \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(self.getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "            \n",
    "        return [wordIndices, caseIndices, charIndices]\n",
    "\n",
    "    def addCharInformation(self, sentence):\n",
    "        return [[word, list(str(word))] for word in sentence]\n",
    "\n",
    "    def padding(self,Sentence):\n",
    "        Sentence[2] = pad_sequences(Sentence[2],52,padding='post')\n",
    "        return Sentence\n",
    "\n",
    "    def predict(self,Sentence):\n",
    "        Sentence = words =  word_tokenize(Sentence)\n",
    "        Sentence = self.addCharInformation(Sentence)\n",
    "        Sentence = self.padding(self.createTensor(Sentence,self.word2Idx,self.case2Idx,self.char2Idx))\n",
    "        tokens, casing,char = Sentence\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = self.model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1)\n",
    "        pred = [self.idx2Label[x].strip() for x in pred]\n",
    "\n",
    "        return  list(zip(words,pred))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Parser()\n",
    "p.load_models(\"model_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "text_file = open(\"Input_sample.txt\").read()\n",
    "token_sent = sent_tokenize(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MÉXICO.—Marcelo Ebrard va a renunciar a la Secretaría de Relaciones Exteriores, según versiones periodísticas que hoy fueron desmentidas por la SRE.', '“En el ámbito de la fantasía”\\n\\nRoberto Velasco Álvarez, vocero de la Cancillería, aseguró a través de Twitter que es totalmente falsa la versión que circuló sobre el tema.', 'El evento referido, señaló, sólo ocurrió en el ámbito de la fantasía, según lo publicado por El Universal.', 'Primero Gertz Moreno\\n\\nAyer lunes también circuló la versión sobre una supuesta renuncia del Fiscal General de la República, Alejandro Gertz Manero, por cuestiones de salud.', 'También fue desmentido por la dependencia.', 'Hoy en importante firma\\n\\nEsta mañana, en Palacio Nacional, con el presidente Andrés Manuel López Obrador como testigo de honor, Michelle Bachelet, alta comisionada de Naciones Unidas para los Derechos Humanos, y el canciller Marcelo Ebrard firmaron el Acuerdo para la formación en materia de derechos humanos y operación de acuerdo a estándares internacionales de derechos humanos a la Guardia Nacional.', 'En el acto, Ebrard dijo que México se perfila hacia un nuevo paradigma de respeto, promoción y protección de los derechos y las libertades fundamentales, que coloca el respeto irrestricto a los derechos humanos en el centro de la política exterior del Gobierno de la República.']\n"
     ]
    }
   ],
   "source": [
    "print(token_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MÉXICO.—Marcelo Ebrard va a renunciar a la Secretaría de Relaciones Exteriores, según versiones periodísticas que hoy fueron desmentidas por la SRE.\n",
      "“En el ámbito de la fantasía”\n",
      "\n",
      "Roberto Velasco Álvarez, vocero de la Cancillería, aseguró a través de Twitter que es totalmente falsa la versión que circuló sobre el tema.\n",
      "\n",
      "El evento referido, señaló, sólo ocurrió en el ámbito de la fantasía, según lo publicado por El Universal.\n",
      "Primero Gertz Moreno\n",
      "\n",
      "Ayer lunes también circuló la versión sobre una supuesta renuncia del Fiscal General de la República, Alejandro Gertz Manero, por cuestiones de salud. También fue desmentido por la dependencia.\n",
      "Hoy en importante firma\n",
      "\n",
      "Esta mañana, en Palacio Nacional, con el presidente Andrés Manuel López Obrador como testigo de honor, Michelle Bachelet, alta comisionada de Naciones Unidas para los Derechos Humanos, y el canciller Marcelo Ebrard firmaron el Acuerdo para la formación en materia de derechos humanos y operación de acuerdo a estándares internacionales de derechos humanos a la Guardia Nacional.\n",
      "\n",
      "En el acto, Ebrard dijo que México se perfila hacia un nuevo paradigma de respeto, promoción y protección de los derechos y las libertades fundamentales, que coloca el respeto irrestricto a los derechos humanos en el centro de la política exterior del Gobierno de la República. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MEXICO.', 'B-ORG')\n",
      "('Marcelo', 'B-PER')\n",
      "('Ebrard', 'I-PER')\n",
      "('Secretaria', 'B-ORG')\n",
      "('de', 'I-ORG')\n",
      "('Relaciones', 'I-MISC')\n",
      "('Exteriores', 'I-MISC')\n",
      "('SRE', 'B-MISC')\n",
      "('Roberto', 'B-PER')\n",
      "('Velasco', 'I-PER')\n",
      "('Alvarez', 'I-PER')\n",
      "('Cancilleria', 'B-PER')\n",
      "('Twitter', 'B-LOC')\n",
      "('senalo', 'I-MISC')\n",
      "('El', 'B-ORG')\n",
      "('Universal', 'I-ORG')\n",
      "('Gertz', 'B-PER')\n",
      "('Moreno', 'I-PER')\n",
      "('Ayer', 'I-PER')\n",
      "('Fiscal', 'B-MISC')\n",
      "('General', 'I-MISC')\n",
      "('de', 'I-ORG')\n",
      "('la', 'I-ORG')\n",
      "('Republica', 'I-ORG')\n",
      "('Alejandro', 'B-PER')\n",
      "('Gertz', 'I-PER')\n",
      "('Manero', 'I-PER')\n",
      "('Tambien', 'B-PER')\n",
      "('Esta', 'B-MISC')\n",
      "('Palacio', 'B-LOC')\n",
      "('Nacional', 'I-LOC')\n",
      "('Andres', 'B-PER')\n",
      "('Manuel', 'I-PER')\n",
      "('Lopez', 'I-PER')\n",
      "('Obrador', 'I-PER')\n",
      "('Michelle', 'I-PER')\n",
      "('Bachelet', 'I-PER')\n",
      "('Naciones', 'B-MISC')\n",
      "('Unidas', 'I-MISC')\n",
      "('para', 'I-ORG')\n",
      "('los', 'I-MISC')\n",
      "('Derechos', 'I-MISC')\n",
      "('Humanos', 'I-MISC')\n",
      "('Marcelo', 'B-PER')\n",
      "('Ebrard', 'I-PER')\n",
      "('Acuerdo', 'B-MISC')\n",
      "('Guardia', 'B-ORG')\n",
      "('Nacional', 'I-ORG')\n",
      "('Ebrard', 'B-MISC')\n",
      "('dijo', 'I-MISC')\n",
      "('Mexico', 'B-PER')\n",
      "('Gobierno', 'B-ORG')\n",
      "('de', 'I-ORG')\n",
      "('la', 'I-ORG')\n",
      "('Republica', 'I-ORG')\n"
     ]
    }
   ],
   "source": [
    "outlist =[]\n",
    "for t in token_sent:\n",
    "    t= unidecode.unidecode(t)\n",
    "    outlist.append(p.predict(t))\n",
    "\n",
    "to_out=[]\n",
    "for s in outlist:\n",
    "    for w in s:\n",
    "        if ('O') not in w:\n",
    "            print(w)\n",
    "            to_out.append(w)\n",
    "            \n",
    "with open('Output_sample.txt', 'w') as f:\n",
    "    for item in to_out:\n",
    "        f.write(\"\\n\")\n",
    "        for x in item:\n",
    "            f.write(\"%s \" %x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
